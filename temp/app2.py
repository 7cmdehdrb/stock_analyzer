from fastapi import FastAPI, Request, HTTPException, UploadFile, File, Form, Depends
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from sqlalchemy import (
    create_engine,
    Column,
    String,
    Float,
    Integer,
    Text,
    DateTime,
    func,
)
from sqlalchemy.orm import sessionmaker, Session, DeclarativeBase
from sqladmin import Admin, ModelView
import pandas as pd
import yfinance as yf
import numpy as np
from datetime import datetime, timedelta
import io
import traceback
import sqlite3
import os
from dotenv import load_dotenv
import openai
from typing import Optional, Dict, Any
import json
from contextlib import asynccontextmanager

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


# Lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ (SQLAlchemy 2.0 í˜¸í™˜)
@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘ ë° ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” ì´ë²¤íŠ¸"""
    # Startup
    print("ğŸš€ Starting Portfolio Analyzer FastAPI App...")
    init_database()
    print("âœ… Database initialized")
    yield
    # Shutdown
    print("ğŸ‘‹ Shutting down Portfolio Analyzer...")


# FastAPI ì•± ìƒì„±
app = FastAPI(title="Portfolio Analyzer", version="2.0", lifespan=lifespan)

# Templates ì„¤ì •
templates = Jinja2Templates(directory="templates")

# OpenAI API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

# AI ë¶„ì„ rate limitingê³¼ ìºì‹±ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
ai_analysis_cache: Dict[str, Dict[str, Any]] = {}
ai_analysis_rate_limit: Dict[str, datetime] = {}

# DB íŒŒì¼ ê²½ë¡œ
DB_PATH = os.path.join(os.path.dirname(__file__), "stock_cache.db")

# SQLAlchemy ì„¤ì •
SQLALCHEMY_DATABASE_URL = f"sqlite:///{DB_PATH}"
engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


# SQLAlchemy Base í´ë˜ìŠ¤ ì •ì˜ (SQLAlchemy 2.0 ìŠ¤íƒ€ì¼)
class Base(DeclarativeBase):
    pass


# SQLAlchemy ëª¨ë¸ ì •ì˜
class StockPriceCache(Base):
    __tablename__ = "stock_price_cache"

    ticker = Column(String(20), primary_key=True)
    date = Column(String(10), primary_key=True)
    close_price = Column(Float, nullable=False)
    created_at = Column(DateTime, default=datetime.now)

    def __repr__(self):
        return f"<StockPrice {self.ticker} {self.date}>"


class SavedPortfolio(Base):
    __tablename__ = "saved_portfolios"

    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String(200), nullable=False)
    csv_content = Column(Text, nullable=False)
    start_date = Column(String(10))
    benchmark_ticker = Column(String(20))
    base_currency = Column(String(3))
    created_at = Column(DateTime, default=datetime.now)
    last_accessed = Column(DateTime, default=datetime.now)

    def __repr__(self):
        return f"<Portfolio {self.name}>"


# SQLAdmin ModelView ì •ì˜
class StockPriceCacheAdmin(ModelView, model=StockPriceCache):
    column_list = [
        StockPriceCache.ticker,
        StockPriceCache.date,
        StockPriceCache.close_price,
        StockPriceCache.created_at,
    ]
    column_searchable_list = [StockPriceCache.ticker]
    column_sortable_list = [
        StockPriceCache.ticker,
        StockPriceCache.date,
        StockPriceCache.created_at,
    ]
    column_default_sort = [
        (StockPriceCache.ticker, False),
        (StockPriceCache.date, True),
    ]
    page_size = 50
    can_export = True


class SavedPortfolioAdmin(ModelView, model=SavedPortfolio):
    column_list = [
        SavedPortfolio.id,
        SavedPortfolio.name,
        SavedPortfolio.start_date,
        SavedPortfolio.benchmark_ticker,
        SavedPortfolio.base_currency,
        SavedPortfolio.created_at,
        SavedPortfolio.last_accessed,
    ]
    column_searchable_list = [SavedPortfolio.name, SavedPortfolio.benchmark_ticker]
    column_sortable_list = [
        SavedPortfolio.id,
        SavedPortfolio.name,
        SavedPortfolio.created_at,
        SavedPortfolio.last_accessed,
    ]
    column_default_sort = [(SavedPortfolio.last_accessed, True)]
    page_size = 50
    can_export = True
    # CSV ë‚´ìš©ì€ ë„ˆë¬´ ê¸¸ì–´ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œì™¸
    column_details_exclude_list = []
    form_excluded_columns = []


# SQLAdmin ì´ˆê¸°í™”
admin = Admin(app, engine)
admin.add_view(StockPriceCacheAdmin)
admin.add_view(SavedPortfolioAdmin)


# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
def init_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° í…Œì´ë¸” ìƒì„±"""
    Base.metadata.create_all(bind=engine)


# Dependency
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ========== í—¬í¼ í•¨ìˆ˜ë“¤ (app.pyì—ì„œ ê°€ì ¸ì˜´) ==========


def get_cached_prices(ticker, start_date, end_date):
    """DBì—ì„œ ìºì‹œëœ ê°€ê²© ë°ì´í„° ì¡°íšŒ"""
    conn = None
    try:
        conn = sqlite3.connect(DB_PATH, timeout=30.0)
        cursor = conn.cursor()

        query = """
            SELECT date, close_price 
            FROM stock_price_cache 
            WHERE ticker = ? AND date >= ? AND date <= ?
            ORDER BY date
        """

        start_str = start_date.strftime("%Y-%m-%d")
        end_str = end_date.strftime("%Y-%m-%d")

        cursor.execute(query, (ticker, start_str, end_str))
        results = cursor.fetchall()

        if not results:
            return None

        # DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(results, columns=["date", "close_price"])
        df["date"] = pd.to_datetime(df["date"])
        df.set_index("date", inplace=True)

        return df["close_price"]
    except Exception as e:
        return None
    finally:
        if conn:
            conn.close()


def save_prices_to_cache(ticker, price_series):
    """ê°€ê²© ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
    if price_series is None or len(price_series) == 0:
        return

    conn = None
    try:
        conn = sqlite3.connect(DB_PATH, timeout=30.0)
        cursor = conn.cursor()

        # ë°ì´í„° ì¤€ë¹„ (NaN ê°’ ì œì™¸)
        data_to_insert = []
        for date, price in price_series.items():
            # NaN ê°’ ì²´í¬
            if pd.notna(price) and not np.isnan(price):
                date_str = date.strftime("%Y-%m-%d")
                data_to_insert.append((ticker, date_str, float(price)))

        if not data_to_insert:
            return

        # INSERT OR REPLACEë¡œ ì¤‘ë³µ ë°©ì§€
        cursor.executemany(
            """
            INSERT OR REPLACE INTO stock_price_cache (ticker, date, close_price)
            VALUES (?, ?, ?)
        """,
            data_to_insert,
        )

        conn.commit()
    except Exception as e:
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()


def get_current_exchange_rate():
    """í˜„ì¬ USD/KRW í™˜ìœ¨ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # USDKRW=X í‹°ì»¤ë¡œ í™˜ìœ¨ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        krw = yf.Ticker("USDKRW=X")
        data = krw.history(period="1d")
        if not data.empty:
            return data["Close"].iloc[-1]
        else:
            # ê¸°ë³¸ê°’ (ìµœê·¼ í‰ê·  í™˜ìœ¨)
            return 1350.0
    except:
        return 1350.0


def fetch_stock_data(ticker, start_date, end_date):
    """Yahoo Financeì—ì„œ ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (DB ìºì‹œ í™œìš©)

    ìƒì¥ì¼ì´ start_dateë³´ë‹¤ ëŠ¦ì€ ê²½ìš°ì—ë„ ìƒì¥ ì´í›„ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ì—¬
    fill_missing_datesì—ì„œ ìƒì¥ì¼ ì´ì „ êµ¬ê°„ì„ ìƒì¥ ì‹œ ê°€ê²©ìœ¼ë¡œ ì±„ìš¸ ìˆ˜ ìˆë„ë¡ í•¨
    """
    try:
        # 1. ë¨¼ì € DB ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ
        cached_data = get_cached_prices(ticker, start_date, end_date)

        # 2. ìºì‹œì— ëª¨ë“  ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if cached_data is not None and len(cached_data) > 0:
            # ë‚ ì§œ ë²”ìœ„ í™•ì¸
            expected_start = pd.to_datetime(start_date)
            expected_end = pd.to_datetime(end_date)
            cached_start = cached_data.index.min()
            cached_end = cached_data.index.max()

            # ìºì‹œê°€ ìš”ì²­ ë²”ìœ„ë¥¼ ëª¨ë‘ ì»¤ë²„í•˜ëŠ”ì§€ í™•ì¸ (Â±7ì¼ í—ˆìš©)
            # ë˜ëŠ” ìƒì¥ì¼ì´ start_date ì´í›„ì¸ ê²½ìš°ì—ë„ ë°ì´í„° ë°˜í™˜
            if cached_start <= expected_start + timedelta(
                days=7
            ) and cached_end >= expected_end - timedelta(days=7):
                return cached_data
            elif cached_end >= expected_end - timedelta(days=7):
                # ìƒì¥ì¼ì´ ëŠ¦ì–´ë„ end_dateê¹Œì§€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°˜í™˜
                return cached_data

        # 3. ìºì‹œì— ì—†ìœ¼ë©´ Yahoo Financeì—ì„œ ê°€ì ¸ì˜¤ê¸°
        stock = yf.Ticker(ticker)
        data = stock.history(start=start_date, end=end_date)

        # ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° (ìƒì¥ì¼ì´ ëŠ¦ì„ ìˆ˜ ìˆìŒ)
        if data.empty:
            # ë” ë„“ì€ ë²”ìœ„ë¡œ ë‹¤ì‹œ ì‹œë„ (ìµœê·¼ 5ë…„ ë˜ëŠ” ìƒì¥ì¼ë¶€í„°)
            print(
                f"  âš  No data for {ticker} in requested range, trying broader range..."
            )
            data = stock.history(period="5y")

            if data.empty:
                print(f"  âŒ {ticker}: Still no data available")
                return None
            else:
                # ìƒì¥ì¼ ì´í›„ ë°ì´í„°ê°€ ìˆìŒ
                listing_date = data.index.min()
                print(
                    f"  â„¹ {ticker} listing date appears to be around {listing_date.date()}"
                )

        price_data = data["Close"]

        # timezone ì œê±° (ìºì‹œëœ ë°ì´í„°ì™€ ì¼ê´€ì„± ìœ ì§€)
        if hasattr(price_data.index, "tz") and price_data.index.tz is not None:
            price_data.index = price_data.index.tz_localize(None)

        # NaN ê°’ ì œê±°
        price_data = price_data.dropna()

        if len(price_data) == 0:
            print(f"  âŒ {ticker}: No valid price data after cleaning")
            return None

        # 4. ìƒˆë¡œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ DBì— ì €ì¥
        save_prices_to_cache(ticker, price_data)

        return price_data

    except Exception as e:
        print(f"  âŒ Error fetching {ticker}: {e}")
        return None


def normalize_ticker(ticker, country):
    """í‹°ì»¤ ì •ê·œí™” (í•œêµ­ ì¢…ëª©ì— .KS ìë™ ì¶”ê°€)"""
    ticker = str(ticker).strip()

    # í•œêµ­ ì¢…ëª©ì¸ ê²½ìš°
    if country == "í•œêµ­":
        # ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì§„ ê²½ìš° (ì˜ˆ: 005930)
        if ticker.isdigit():
            ticker = f"{ticker}.KS"
            return ticker
        # ì´ë¯¸ .KSë‚˜ .KQê°€ ë¶™ì–´ìˆì§€ ì•Šì€ ê²½ìš°
        elif not (ticker.endswith(".KS") or ticker.endswith(".KQ")):
            ticker = f"{ticker}.KS"
            return ticker

    return ticker


def merge_duplicate_tickers(portfolio_df):
    """ë™ì¼ í‹°ì»¤ë¥¼ ê°€ì§„ ì¢…ëª©ì˜ ë³´ìœ ëŸ‰ì„ í•©ì‚°"""
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    if "í‹°ì»¤" not in portfolio_df.columns or "ë³´ìœ ëŸ‰" not in portfolio_df.columns:
        return portfolio_df

    # í‹°ì»¤ ì •ê·œí™” (í•œêµ­ ì¢…ëª©ì— .KS ì¶”ê°€)
    if "êµ­ê°€" in portfolio_df.columns:
        portfolio_df["í‹°ì»¤"] = portfolio_df.apply(
            lambda row: normalize_ticker(row["í‹°ì»¤"], row.get("êµ­ê°€", "")), axis=1
        )

    # í‹°ì»¤ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë³´ìœ ëŸ‰ í•©ì‚°
    # ì²« ë²ˆì§¸ í–‰ì˜ ë‹¤ë¥¸ ì •ë³´(ì¢…ëª©ëª…, êµ­ê°€, ë¶„ë¥˜ ë“±)ëŠ” ìœ ì§€
    grouped = portfolio_df.groupby("í‹°ì»¤", as_index=False).agg(
        {
            "ë³´ìœ ëŸ‰": "sum",  # ë³´ìœ ëŸ‰ì€ í•©ì‚°
            **{
                col: "first"
                for col in portfolio_df.columns
                if col not in ["í‹°ì»¤", "ë³´ìœ ëŸ‰"]
            },
        }
    )

    return grouped


def fill_missing_dates(price_series, start_date, end_date):
    """íœ´ì¥ì¼ ë° ìƒì¥ì¼ë¡œ ì¸í•œ ë¹ˆ ë°ì´í„° ì²˜ë¦¬

    Args:
        price_series: ì£¼ê°€ ì‹œê³„ì—´ ë°ì´í„°
        start_date: ë¶„ì„ ì‹œì‘ ë‚ ì§œ
        end_date: ë¶„ì„ ì¢…ë£Œ ë‚ ì§œ

    Returns:
        ë³´ê°„ëœ ì£¼ê°€ ì‹œê³„ì—´ ë°ì´í„°
    """
    if price_series is None or len(price_series) == 0:
        print("  âš  fill_missing_dates: No data provided")
        return None

    try:
        print(f"  ğŸ“… Date range: {start_date.date()} to {end_date.date()}")
        print(f"  ğŸ“Š Original data: {len(price_series)} trading days")
        print(
            f"  ğŸ” Index type: {type(price_series.index)}, dtype: {price_series.index.dtype}"
        )

        # ì›ë³¸ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ë¥¼ timezone-naiveë¡œ ë³€í™˜
        if hasattr(price_series.index, "tz") and price_series.index.tz is not None:
            print(f"  ğŸŒ Converting from timezone: {price_series.index.tz}")
            price_series.index = price_series.index.tz_localize(None)

        # ì¸ë±ìŠ¤ê°€ DatetimeIndexì¸ì§€ í™•ì¸
        if not isinstance(price_series.index, pd.DatetimeIndex):
            print(f"  ğŸ”„ Converting index to DatetimeIndex")
            price_series.index = pd.to_datetime(price_series.index)

        # ì „ì²´ ë‚ ì§œ ë²”ìœ„ ìƒì„± (ëª¨ë“  ë‚ ì§œ í¬í•¨)
        all_dates = pd.date_range(start=start_date, end=end_date, freq="D")

        print(f"  ğŸ“† Expanded range: {len(all_dates)} days")
        print(
            f"  ğŸ“Š First original date: {price_series.index[0]}, Last: {price_series.index[-1]}"
        )

        # ê¸°ì¡´ ë°ì´í„°ë¥¼ ì „ì²´ ë‚ ì§œ ë²”ìœ„ë¡œ í™•ì¥
        price_series_filled = price_series.reindex(all_dates)

        # ë°ì´í„°ê°€ ìˆëŠ” ì²« ë‚ ì§œ í™•ì¸ (ìƒì¥ì¼)
        first_valid_date = price_series_filled.first_valid_index()

        if first_valid_date is None:
            print("  âŒ No valid dates found after reindex")
            print(
                f"  ğŸ” Checking overlap: orig min={price_series.index.min()}, max={price_series.index.max()}"
            )
            print(
                f"  ğŸ” Checking overlap: new min={all_dates.min()}, max={all_dates.max()}"
            )
            # ê·¸ëƒ¥ ì›ë³¸ ë°ì´í„° ë°˜í™˜ (ë‚ ì§œ í™•ì¥ ì—†ì´)
            return price_series

        # ìƒì¥ ì´ì „ ë°ì´í„°: ìƒì¥ í›„ ì²« ê°€ê²©ìœ¼ë¡œ ì±„ì›€
        first_price = price_series_filled[first_valid_date]
        price_series_filled.loc[:first_valid_date] = first_price

        # ìƒì¥ ì´í›„ ë¹ˆ ë°ì´í„°: ì„ í˜• ë³´ê°„
        price_series_filled = price_series_filled.interpolate(
            method="linear", limit_direction="forward"
        )

        # ì•„ì§ë„ ë¹ˆ ê°’ì´ ìˆë‹¤ë©´ (ë ë¶€ë¶„) forward fill
        price_series_filled = price_series_filled.ffill()

        # ê·¸ë˜ë„ ë‚¨ì•„ìˆëŠ” NaNì€ backward fill
        price_series_filled = price_series_filled.bfill()

        filled_count = len(all_dates) - len(price_series)
        print(
            f"  âœ“ Filled {filled_count} missing dates (total: {len(price_series_filled)} days)"
        )

        return price_series_filled

    except Exception as e:
        print(f"  âŒ Error in fill_missing_dates: {e}")
        import traceback

        traceback.print_exc()
        # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë°ì´í„° ë°˜í™˜
        return price_series


def calculate_portfolio_returns(portfolio_df, start_date, base_currency="USD"):
    """í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê³„ì‚° (ê¸°ì¤€ í†µí™” ì ìš©, í˜„ê¸ˆ ì œì™¸)"""
    end_date = datetime.now()

    # í˜„ì¬ í™˜ìœ¨ ê°€ì ¸ì˜¤ê¸°
    exchange_rate = get_current_exchange_rate()
    print(f"Current USD/KRW exchange rate: {exchange_rate}")

    # ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥  ë°ì´í„° ìˆ˜ì§‘
    portfolio_data = {}
    cash_holdings = {}
    failed_tickers = []  # ì‹¤íŒ¨í•œ í‹°ì»¤ ì¶”ì 
    total_initial_value = 0
    total_initial_value_with_cash = 0

    for _, row in portfolio_df.iterrows():
        ticker = row["í‹°ì»¤"]
        quantity = row["ë³´ìœ ëŸ‰"]
        country = row.get("êµ­ê°€", "ë¯¸êµ­")  # ê¸°ë³¸ê°’ì€ ë¯¸êµ­
        asset_class = row.get("ë¶„ë¥˜", "")

        # í˜„ê¸ˆì¸ ê²½ìš° ë³„ë„ ì²˜ë¦¬
        if asset_class == "í˜„ê¸ˆ":
            # í˜„ê¸ˆ ê°€ì¹˜ ê³„ì‚° (í™˜ìœ¨ ì ìš©)
            if country == "í•œêµ­" and base_currency == "USD":
                cash_value = quantity / exchange_rate
            elif country == "ë¯¸êµ­" and base_currency == "KRW":
                cash_value = quantity * exchange_rate
            else:
                cash_value = quantity

            cash_holdings[ticker] = {
                "value": cash_value,
                "country": country,
                "ticker": ticker,
            }
            total_initial_value_with_cash += cash_value
            continue

        # ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        price_data = fetch_stock_data(ticker, start_date, end_date)

        if price_data is None or len(price_data) == 0:
            print(f"âš  Skipping {ticker}: No price data available")
            failed_tickers.append(ticker)  # ì‹¤íŒ¨í•œ í‹°ì»¤ ê¸°ë¡
            continue

        # fill_missing_datesë¥¼ í˜¸ì¶œí•˜ì—¬ ìƒì¥ì¼ ì´ì „ ë°ì´í„°ë¥¼ ìƒì¥ ì‹œ ê°€ê²©ìœ¼ë¡œ ì±„ì›€
        print(f"  ğŸ”„ Filling missing dates for {ticker}...")
        price_data = fill_missing_dates(price_data, start_date, end_date)

        if price_data is None or len(price_data) == 0:
            print(f"âš  Skipping {ticker}: Failed to process price data")
            failed_tickers.append(ticker)
            continue

        # í™˜ìœ¨ ì ìš©
        # ê¸°ì¤€ í†µí™”ê°€ USDì´ê³  í•œêµ­ ì£¼ì‹ì¸ ê²½ìš° -> USDë¡œ í™˜ì‚°
        # ê¸°ì¤€ í†µí™”ê°€ KRWì´ê³  ë¯¸êµ­ ì£¼ì‹ì¸ ê²½ìš° -> KRWë¡œ í™˜ì‚°
        if base_currency == "USD" and country == "í•œêµ­":
            # í•œêµ­ ì£¼ì‹ì„ USDë¡œ í™˜ì‚° (KRW / í™˜ìœ¨)
            price_data = price_data / exchange_rate
        elif base_currency == "KRW" and country == "ë¯¸êµ­":
            # ë¯¸êµ­ ì£¼ì‹ì„ KRWë¡œ í™˜ì‚° (USD * í™˜ìœ¨)
            price_data = price_data * exchange_rate

        initial_price = price_data.iloc[0]
        initial_value = initial_price * quantity
        total_initial_value += initial_value
        total_initial_value_with_cash += initial_value

        portfolio_data[ticker] = {
            "prices": price_data,
            "quantity": quantity,
            "initial_value": initial_value,
            "country": country,
            "asset_class": asset_class,
            "name": row.get("ì¢…ëª©ëª…", ticker),
        }
        print(f"âœ“ Added {ticker} to portfolio")

    if not portfolio_data:
        print(f"âŒ No valid portfolio data found. Cash holdings: {len(cash_holdings)}")
        return (
            None,
            None,
            None,
            cash_holdings,
            total_initial_value_with_cash,
            failed_tickers,
        )

    # ëª¨ë“  ë‚ ì§œì˜ í•©ì§‘í•© êµ¬í•˜ê¸° (start_date ì´í›„ë§Œ)
    all_dates = pd.DatetimeIndex([])

    # start_dateë¥¼ timezone-naiveë¡œ ë³€í™˜ (fetch_stock_dataê°€ timezone ì—†ëŠ” ë°ì´í„° ë°˜í™˜)
    start_date_tz = pd.to_datetime(start_date)

    for data in portfolio_data.values():
        prices_index = data["prices"].index

        # start_date ì´í›„ ë°ì´í„°ë§Œ ì‚¬ìš©
        ticker_dates = prices_index[prices_index >= start_date_tz]
        all_dates = all_dates.union(ticker_dates)

    all_dates = sorted(all_dates)

    print(f"\nğŸ“… Portfolio date range:")
    print(f"  Start date requested: {start_date.date()}")
    print(
        f"  Actual start date: {all_dates[0].date() if len(all_dates) > 0 else 'N/A'}"
    )
    print(f"  End date: {all_dates[-1].date() if len(all_dates) > 0 else 'N/A'}")
    print(f"  Total trading days: {len(all_dates)}")

    # í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ ê°€ì¹˜ ê³„ì‚°
    portfolio_values = []

    for date in all_dates:
        daily_value = 0
        for ticker, data in portfolio_data.items():
            # í•´ë‹¹ ë‚ ì§œì˜ ê°€ê²© (ì—†ìœ¼ë©´ forward fill)
            if date in data["prices"].index:
                price = data["prices"][date]
            else:
                # ê°€ì¥ ìµœê·¼ ê°€ê²© ì‚¬ìš©
                available_prices = data["prices"][data["prices"].index <= date]
                if len(available_prices) > 0:
                    price = available_prices.iloc[-1]
                else:
                    price = data["prices"].iloc[0]

            daily_value += price * data["quantity"]

        portfolio_values.append(daily_value)

    portfolio_series = pd.Series(portfolio_values, index=all_dates)

    print(f"\nğŸ’° Portfolio values:")
    print(f"  Initial value: ${portfolio_series.iloc[0]:,.2f}")
    print(f"  Final value: ${portfolio_series.iloc[-1]:,.2f}")
    print(
        f"  Total return: {(portfolio_series.iloc[-1] / portfolio_series.iloc[0] - 1) * 100:.2f}%"
    )

    # ì¼ì¼ ìˆ˜ìµë¥  ê³„ì‚°
    returns = portfolio_series.pct_change().dropna()

    return (
        returns,
        portfolio_data,
        portfolio_series,
        cash_holdings,
        total_initial_value_with_cash,
        failed_tickers,
    )


def calculate_weighted_annual_return(portfolio_returns):
    """ì—°í™˜ì‚° ìˆ˜ìµë¥  ê³„ì‚° (ë³µë¦¬)"""
    try:
        if len(portfolio_returns) == 0:
            return 0.0

        # ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°
        cumulative_return = (1 + portfolio_returns).prod() - 1

        # ê±°ë˜ì¼ ìˆ˜
        trading_days = len(portfolio_returns)

        # ì—°í™˜ì‚° (252 ê±°ë˜ì¼ ê¸°ì¤€)
        annual_return = (1 + cumulative_return) ** (252 / trading_days) - 1

        return annual_return * 100  # í¼ì„¼íŠ¸ë¡œ ë°˜í™˜

    except Exception as e:
        print(f"Error calculating annual return: {e}")
        return 0.0


def calculate_metrics(portfolio_returns, benchmark_returns):
    """í¬íŠ¸í´ë¦¬ì˜¤ ì„±ê³¼ ì§€í‘œ ê³„ì‚°"""

    metrics = {}

    try:
        # 1. ì—°í™˜ì‚° ìˆ˜ìµë¥ 
        metrics["annual_return"] = calculate_weighted_annual_return(portfolio_returns)
        metrics["benchmark_annual_return"] = calculate_weighted_annual_return(
            benchmark_returns
        )

        # 2. ëˆ„ì  ìˆ˜ìµë¥ 
        portfolio_cumulative = (1 + portfolio_returns).prod() - 1
        benchmark_cumulative = (1 + benchmark_returns).prod() - 1
        metrics["cumulative_return"] = portfolio_cumulative * 100
        metrics["benchmark_cumulative_return"] = benchmark_cumulative * 100

        # 3. ë³€ë™ì„± (ì—°í™˜ì‚°)
        portfolio_volatility = portfolio_returns.std() * np.sqrt(252)
        benchmark_volatility = benchmark_returns.std() * np.sqrt(252)
        metrics["volatility"] = portfolio_volatility * 100
        metrics["benchmark_volatility"] = benchmark_volatility * 100

        # 4. ìƒ¤í”„ ë¹„ìœ¨ (ë¬´ìœ„í—˜ ìˆ˜ìµë¥  0% ê°€ì •)
        if portfolio_volatility > 0:
            sharpe_ratio = (metrics["annual_return"] / 100) / portfolio_volatility
            metrics["sharpe_ratio"] = sharpe_ratio
        else:
            metrics["sharpe_ratio"] = 0

        if benchmark_volatility > 0:
            benchmark_sharpe = (
                metrics["benchmark_annual_return"] / 100
            ) / benchmark_volatility
            metrics["benchmark_sharpe_ratio"] = benchmark_sharpe
        else:
            metrics["benchmark_sharpe_ratio"] = 0

        # 5. ìµœëŒ€ ë‚™í­ (Maximum Drawdown)
        portfolio_cum_returns = (1 + portfolio_returns).cumprod()
        portfolio_running_max = portfolio_cum_returns.expanding().max()
        portfolio_drawdown = (
            portfolio_cum_returns - portfolio_running_max
        ) / portfolio_running_max
        metrics["max_drawdown"] = portfolio_drawdown.min() * 100

        benchmark_cum_returns = (1 + benchmark_returns).cumprod()
        benchmark_running_max = benchmark_cum_returns.expanding().max()
        benchmark_drawdown = (
            benchmark_cum_returns - benchmark_running_max
        ) / benchmark_running_max
        metrics["benchmark_max_drawdown"] = benchmark_drawdown.min() * 100

        # 6. ë² íƒ€ (Beta)
        # ê³µë¶„ì‚° / ë²¤ì¹˜ë§ˆí¬ ë¶„ì‚°
        covariance = portfolio_returns.cov(benchmark_returns)
        benchmark_variance = benchmark_returns.var()

        if benchmark_variance > 0:
            beta = covariance / benchmark_variance
            metrics["beta"] = beta
        else:
            metrics["beta"] = 0

        # 7. ì•ŒíŒŒ (Alpha) - ì—°í™˜ì‚°
        # ì•ŒíŒŒ = í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  - (ë¬´ìœ„í—˜ ìˆ˜ìµë¥  + ë² íƒ€ * (ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥  - ë¬´ìœ„í—˜ ìˆ˜ìµë¥ ))
        # ë¬´ìœ„í—˜ ìˆ˜ìµë¥  = 0ìœ¼ë¡œ ê°€ì •
        if "beta" in metrics:
            alpha = metrics["annual_return"] - (
                metrics["beta"] * metrics["benchmark_annual_return"]
            )
            metrics["alpha"] = alpha
        else:
            metrics["alpha"] = 0

        # 8. ì •ë³´ ë¹„ìœ¨ (Information Ratio)
        # (í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  - ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥ ) / ì¶”ì ì˜¤ì°¨
        excess_returns = portfolio_returns - benchmark_returns
        tracking_error = excess_returns.std() * np.sqrt(252)

        if tracking_error > 0:
            information_ratio = (
                metrics["annual_return"] - metrics["benchmark_annual_return"]
            ) / (tracking_error * 100)
            metrics["information_ratio"] = information_ratio
        else:
            metrics["information_ratio"] = 0

        # 9. ìŠ¹ë¥  (Win Rate) - ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„
        outperformance_days = (portfolio_returns > benchmark_returns).sum()
        total_days = len(portfolio_returns)
        metrics["win_rate"] = (
            (outperformance_days / total_days * 100) if total_days > 0 else 0
        )

        # 10. ì†Œí‹°ë…¸ ë¹„ìœ¨ (Sortino Ratio)
        # ìƒ¤í”„ ë¹„ìœ¨ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ í•˜ë°© ë³€ë™ì„±ë§Œ ê³ ë ¤
        downside_returns = portfolio_returns[portfolio_returns < 0]
        if len(downside_returns) > 0:
            downside_std = downside_returns.std() * np.sqrt(252)
            if downside_std > 0:
                sortino_ratio = (metrics["annual_return"] / 100) / downside_std
                metrics["sortino_ratio"] = sortino_ratio
            else:
                metrics["sortino_ratio"] = 0
        else:
            metrics["sortino_ratio"] = 0

        # 11. ì¹¼ë§ˆ ë¹„ìœ¨ (Calmar Ratio)
        # ì—°í™˜ì‚° ìˆ˜ìµë¥  / ì ˆëŒ€ê°’(ìµœëŒ€ ë‚™í­)
        if metrics["max_drawdown"] != 0:
            calmar_ratio = (metrics["annual_return"] / 100) / abs(
                metrics["max_drawdown"] / 100
            )
            metrics["calmar_ratio"] = calmar_ratio
        else:
            metrics["calmar_ratio"] = 0

        # ëª¨ë“  ê°’ì„ Python floatë¡œ ë³€í™˜ (numpy íƒ€ì… ì œê±°)
        metrics = {
            k: float(v) if isinstance(v, (np.integer, np.floating)) else v
            for k, v in metrics.items()
        }

        return metrics

    except Exception as e:
        print(f"Error calculating metrics: {e}")
        traceback.print_exc()
        return {}


def prepare_chart_data(portfolio_returns, benchmark_returns, portfolio_series):
    """ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„"""
    try:
        # ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°
        portfolio_cumulative = (1 + portfolio_returns).cumprod()
        benchmark_cumulative = (1 + benchmark_returns).cumprod()

        # ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        dates = [date.strftime("%Y-%m-%d") for date in portfolio_cumulative.index]

        chart_data = {
            "dates": dates,
            "portfolio": [
                float(val - 1) * 100 for val in portfolio_cumulative.values
            ],  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
            "benchmark": [float(val - 1) * 100 for val in benchmark_cumulative.values],
        }

        return chart_data

    except Exception as e:
        print(f"Error preparing chart data: {e}")
        traceback.print_exc()
        return None


def prepare_allocation_data(
    portfolio_data, cash_holdings, base_currency, exchange_rate
):
    """ìì‚° ë°°ë¶„ ë°ì´í„° ì¤€ë¹„ (êµ­ê°€ë³„, ìì‚°ë³„)"""
    try:
        country_allocation = {}
        asset_allocation = {}

        # ì£¼ì‹ ìì‚°
        for ticker, data in portfolio_data.items():
            country = data["country"]
            asset_class = data.get("asset_class", "ì£¼ì‹")
            current_value = data["prices"].iloc[-1] * data["quantity"]

            # êµ­ê°€ë³„ ì§‘ê³„
            if country in country_allocation:
                country_allocation[country] += current_value
            else:
                country_allocation[country] = current_value

            # ìì‚°ë³„ ì§‘ê³„
            if asset_class in asset_allocation:
                asset_allocation[asset_class] += current_value
            else:
                asset_allocation[asset_class] = current_value

        # í˜„ê¸ˆ ìì‚° ì¶”ê°€
        for ticker, cash_data in cash_holdings.items():
            country = cash_data["country"]
            cash_value = cash_data["value"]

            # êµ­ê°€ë³„ ì§‘ê³„
            if country in country_allocation:
                country_allocation[country] += cash_value
            else:
                country_allocation[country] = cash_value

            # ìì‚°ë³„ ì§‘ê³„
            if "í˜„ê¸ˆ" in asset_allocation:
                asset_allocation["í˜„ê¸ˆ"] += cash_value
            else:
                asset_allocation["í˜„ê¸ˆ"] = cash_value

        # ìƒìœ„ Nê°œë§Œ í‘œì‹œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜
        def get_top_allocations(allocation_dict, top_n=10):
            sorted_items = sorted(
                allocation_dict.items(), key=lambda x: x[1], reverse=True
            )
            if len(sorted_items) <= top_n:
                # floatë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
                return {k: float(v) for k, v in sorted_items}
            else:
                top_items = {k: float(v) for k, v in sorted_items[:top_n]}
                others_value = sum(value for _, value in sorted_items[top_n:])
                if others_value > 0:
                    top_items["ê¸°íƒ€"] = float(others_value)
                return top_items

        # ìƒìœ„ ìì‚°ë§Œ í•„í„°ë§
        country_allocation = get_top_allocations(country_allocation, top_n=10)
        asset_allocation = get_top_allocations(asset_allocation, top_n=10)

        return {
            "country": country_allocation,
            "asset": asset_allocation,
        }

    except Exception as e:
        print(f"Error preparing allocation data: {e}")
        traceback.print_exc()
        return {"country": {}, "asset": {}}


def prepare_holdings_table(portfolio_data, cash_holdings, base_currency, exchange_rate):
    """ë³´ìœ  ì¢…ëª© í…Œì´ë¸” ë°ì´í„° ì¤€ë¹„"""
    try:
        holdings = []

        # ì£¼ì‹ ìì‚°
        for ticker, data in portfolio_data.items():
            initial_price = float(data["prices"].iloc[0])
            current_price = float(data["prices"].iloc[-1])
            quantity = float(data["quantity"])

            initial_value = initial_price * quantity
            current_value = current_price * quantity
            gain_loss = current_value - initial_value
            gain_loss_pct = (
                (gain_loss / initial_value * 100) if initial_value > 0 else 0
            )

            holdings.append(
                {
                    "ticker": str(ticker),
                    "name": str(data.get("name", ticker)),
                    "quantity": float(quantity),
                    "initial_price": float(initial_price),
                    "current_price": float(current_price),
                    "initial_value": float(initial_value),
                    "current_value": float(current_value),
                    "gain_loss": float(gain_loss),
                    "gain_loss_pct": float(gain_loss_pct),
                    "country": str(data["country"]),
                    "asset_class": str(data.get("asset_class", "ì£¼ì‹")),
                }
            )

        # í˜„ê¸ˆ ìì‚° ì¶”ê°€
        for ticker, cash_data in cash_holdings.items():
            holdings.append(
                {
                    "ticker": str(ticker),
                    "name": "í˜„ê¸ˆ",
                    "quantity": 1.0,
                    "initial_price": float(cash_data["value"]),
                    "current_price": float(cash_data["value"]),
                    "initial_value": float(cash_data["value"]),
                    "current_value": float(cash_data["value"]),
                    "gain_loss": 0.0,
                    "gain_loss_pct": 0.0,
                    "country": str(cash_data["country"]),
                    "asset_class": "í˜„ê¸ˆ",
                }
            )

        # í˜„ì¬ ê°€ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        holdings.sort(key=lambda x: x["current_value"], reverse=True)

        return holdings

    except Exception as e:
        print(f"Error preparing holdings table: {e}")
        traceback.print_exc()
        return []


# ========== FastAPI ë¼ìš°íŠ¸ ==========


@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    """ë©”ì¸ í˜ì´ì§€"""
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/api/cache-stats")
async def get_cache_stats(db: Session = Depends(get_db)):
    """ìºì‹œ í†µê³„ API"""
    try:
        # í‹°ì»¤ë³„ ìºì‹œ ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ
        result = (
            db.query(
                StockPriceCache.ticker,
                func.count(StockPriceCache.date).label("count"),
                func.min(StockPriceCache.date).label("min_date"),
                func.max(StockPriceCache.date).label("max_date"),
            )
            .group_by(StockPriceCache.ticker)
            .all()
        )

        cache_stats = []
        for row in result:
            cache_stats.append(
                {
                    "ticker": row.ticker,
                    "count": row.count,
                    "min_date": row.min_date,
                    "max_date": row.max_date,
                }
            )

        return JSONResponse(
            content={
                "success": True,
                "total_tickers": len(cache_stats),
                "cache_stats": cache_stats,
            }
        )

    except Exception as e:
        print(f"Error in get_cache_stats: {e}")
        traceback.print_exc()
        return JSONResponse(
            status_code=500, content={"success": False, "error": str(e)}
        )


@app.post("/api/save-portfolio")
async def save_portfolio(
    name: str = Form(...),
    csv_content: str = Form(...),
    start_date: str = Form(...),
    benchmark_ticker: str = Form(...),
    base_currency: str = Form(...),
    db: Session = Depends(get_db),
):
    """í¬íŠ¸í´ë¦¬ì˜¤ ì €ì¥ API"""
    try:
        # ìƒˆ í¬íŠ¸í´ë¦¬ì˜¤ ìƒì„±
        portfolio = SavedPortfolio(
            name=name,
            csv_content=csv_content,
            start_date=start_date,
            benchmark_ticker=benchmark_ticker,
            base_currency=base_currency,
        )

        db.add(portfolio)
        db.commit()
        db.refresh(portfolio)

        return JSONResponse(
            content={
                "success": True,
                "portfolio_id": portfolio.id,
                "message": "í¬íŠ¸í´ë¦¬ì˜¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
            }
        )

    except Exception as e:
        db.rollback()
        return JSONResponse(
            status_code=500, content={"success": False, "error": str(e)}
        )


@app.get("/ranking", response_class=HTMLResponse)
async def ranking_page(request: Request):
    """ë­í‚¹ í˜ì´ì§€"""
    return templates.TemplateResponse("ranking.html", {"request": request})


@app.get("/api/rankings")
async def get_rankings(db: Session = Depends(get_db)):
    """í¬íŠ¸í´ë¦¬ì˜¤ ë­í‚¹ ì¡°íšŒ API"""
    try:
        portfolios = db.query(SavedPortfolio).all()

        if not portfolios:
            return JSONResponse(content={"success": True, "rankings": []})

        # ê° í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„
        rankings = []
        for portfolio in portfolios:
            try:
                # CSV íŒŒì‹±
                csv_data = io.StringIO(portfolio.csv_content)
                portfolio_df = pd.read_csv(csv_data)

                # ì¤‘ë³µ í‹°ì»¤ ë³‘í•©
                portfolio_df = merge_duplicate_tickers(portfolio_df)

                # ì‹œì‘ ë‚ ì§œ íŒŒì‹±
                start_date_obj = datetime.strptime(portfolio.start_date, "%Y-%m-%d")

                # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê³„ì‚°
                (
                    portfolio_returns,
                    portfolio_data,
                    portfolio_series,
                    cash_holdings,
                    total_initial_value_with_cash,
                    failed_tickers,
                ) = calculate_portfolio_returns(
                    portfolio_df, start_date_obj, portfolio.base_currency
                )

                if portfolio_returns is None:
                    continue

                # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°
                benchmark_data = fetch_stock_data(
                    portfolio.benchmark_ticker, start_date_obj, datetime.now()
                )

                if benchmark_data is None:
                    continue

                # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë„ fill_missing_dates í˜¸ì¶œ
                benchmark_data = fill_missing_dates(
                    benchmark_data, start_date_obj, datetime.now()
                )

                if benchmark_data is None or len(benchmark_data) == 0:
                    continue

                start_date_for_filter = pd.to_datetime(start_date_obj)
                benchmark_data = benchmark_data[
                    benchmark_data.index >= start_date_for_filter
                ]
                benchmark_returns = benchmark_data.pct_change().dropna()

                # ì§€í‘œ ê³„ì‚°
                metrics = calculate_metrics(portfolio_returns, benchmark_returns)

                rankings.append(
                    {
                        "id": portfolio.id,
                        "name": portfolio.name,
                        "annual_return": metrics.get("annual_return", 0),
                        "cumulative_return": metrics.get("cumulative_return", 0),
                        "sharpe_ratio": metrics.get("sharpe_ratio", 0),
                        "max_drawdown": metrics.get("max_drawdown", 0),
                        "volatility": metrics.get("volatility", 0),
                        "alpha": metrics.get("alpha", 0),
                        "beta": metrics.get("beta", 0),
                        "start_date": portfolio.start_date,
                        "benchmark_ticker": portfolio.benchmark_ticker,
                        "created_at": portfolio.created_at.strftime(
                            "%Y-%m-%d %H:%M:%S"
                        ),
                    }
                )

            except Exception as e:
                print(f"Error processing portfolio {portfolio.id}: {e}")
                continue

        # ì—°í™˜ì‚° ìˆ˜ìµë¥  ê¸°ì¤€ ì •ë ¬
        rankings.sort(key=lambda x: x["annual_return"], reverse=True)

        return JSONResponse(content={"success": True, "rankings": rankings})

    except Exception as e:
        return JSONResponse(
            status_code=500, content={"success": False, "error": str(e)}
        )


@app.get("/portfolio/{portfolio_id}", response_class=HTMLResponse)
async def portfolio_view(
    request: Request, portfolio_id: int, db: Session = Depends(get_db)
):
    """ì €ì¥ëœ í¬íŠ¸í´ë¦¬ì˜¤ ì¡°íšŒ"""
    try:
        portfolio = (
            db.query(SavedPortfolio).filter(SavedPortfolio.id == portfolio_id).first()
        )

        if not portfolio:
            raise HTTPException(
                status_code=404, detail="í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        # ë§ˆì§€ë§‰ ì ‘ì† ì‹œê°„ ì—…ë°ì´íŠ¸
        portfolio.last_accessed = datetime.now()
        db.commit()

        return templates.TemplateResponse(
            "portfolio_view.html",
            {
                "request": request,
                "portfolio_id": portfolio_id,
                "portfolio_name": portfolio.name,
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/analyze")
async def analyze_portfolio(
    file: UploadFile = File(...),
    start_date: str = Form(...),
    benchmark_ticker: str = Form(...),
    base_currency: str = Form("USD"),
):
    """í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ API"""
    try:
        print("=" * 80)
        print("ğŸ“Š Starting portfolio analysis...")
        print(f"  Start date: {start_date}")
        print(f"  Benchmark: {benchmark_ticker}")
        print(f"  Base currency: {base_currency}")
        print("=" * 80)

        # CSV íŒŒì¼ ì½ê¸°
        contents = await file.read()
        csv_data = io.StringIO(contents.decode("utf-8"))
        portfolio_df = pd.read_csv(csv_data)

        print(f"\nğŸ“‹ Portfolio data loaded:")
        print(f"  Rows: {len(portfolio_df)}")
        print(f"  Columns: {list(portfolio_df.columns)}")

        # ì¤‘ë³µ í‹°ì»¤ ë³‘í•©
        portfolio_df = merge_duplicate_tickers(portfolio_df)
        print(f"  After merging duplicates: {len(portfolio_df)} unique tickers")

        # ì‹œì‘ ë‚ ì§œ íŒŒì‹±
        try:
            start_date_obj = datetime.strptime(start_date, "%Y-%m-%d")
        except ValueError:
            return JSONResponse(
                status_code=400,
                content={
                    "error": "ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”."
                },
            )

        # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê³„ì‚°
        print(f"\nğŸ’¼ Calculating portfolio returns...")
        (
            portfolio_returns,
            portfolio_data,
            portfolio_series,
            cash_holdings,
            total_initial_value_with_cash,
            failed_tickers,
        ) = calculate_portfolio_returns(portfolio_df, start_date_obj, base_currency)

        if portfolio_returns is None:
            return JSONResponse(
                status_code=400,
                content={
                    "error": "í¬íŠ¸í´ë¦¬ì˜¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CSV íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
                },
            )

        warning_msg = ""
        if failed_tickers:
            warning_msg = f"âš ï¸ ë‹¤ìŒ í‹°ì»¤ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(failed_tickers)}"
            print(f"âš ï¸ Warning: Some tickers failed: {failed_tickers}")

        # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        print(f"ğŸ“Š Fetching benchmark data: {benchmark_ticker}")
        benchmark_data = fetch_stock_data(
            benchmark_ticker, start_date_obj, datetime.now()
        )

        if benchmark_data is None:
            print(f"âŒ Failed to fetch benchmark data for {benchmark_ticker}")
            return JSONResponse(
                status_code=400,
                content={
                    "error": f'ë²¤ì¹˜ë§ˆí¬ í‹°ì»¤ "{benchmark_ticker}"ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‹°ì»¤ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.'
                },
            )

        # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë„ fill_missing_dates í˜¸ì¶œ
        print(f"  ğŸ”„ Filling missing dates for benchmark {benchmark_ticker}...")
        benchmark_data = fill_missing_dates(
            benchmark_data, start_date_obj, datetime.now()
        )

        if benchmark_data is None or len(benchmark_data) == 0:
            print(f"âŒ Failed to process benchmark data for {benchmark_ticker}")
            return JSONResponse(
                status_code=400,
                content={"error": f"ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."},
            )

        # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë„ start_date ì´í›„ë§Œ ì‚¬ìš©
        print(f"ğŸ“Š Benchmark data before filter: {len(benchmark_data)} days")

        # fetch_stock_dataê°€ timezone ì—†ëŠ” ë°ì´í„° ë°˜í™˜í•˜ë¯€ë¡œ ë‹¨ìˆœ ë¹„êµ
        start_date_for_filter = pd.to_datetime(start_date_obj)

        benchmark_data = benchmark_data[benchmark_data.index >= start_date_for_filter]
        print(f"ğŸ“Š Benchmark data after filter: {len(benchmark_data)} days")
        if len(benchmark_data) > 0:
            print(
                f"    Date range: {benchmark_data.index[0].date()} to {benchmark_data.index[-1].date()}"
            )
        else:
            print(f"    âŒ No benchmark data after filtering!")
            return JSONResponse(
                status_code=400,
                content={
                    "error": f"ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ê°€ ì‹œì‘ ë‚ ì§œ({start_date}) ì´í›„ì— ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œë¥¼ í™•ì¸í•˜ì„¸ìš”."
                },
            )

        benchmark_returns = benchmark_data.pct_change().dropna()

        print(f"ğŸ“Š Returns comparison:")
        print(f"  Portfolio returns: {len(portfolio_returns)} days")
        print(f"  Benchmark returns: {len(benchmark_returns)} days")

        if len(benchmark_returns) == 0:
            print(f"    âŒ No benchmark returns after pct_change!")
            return JSONResponse(
                status_code=400,
                content={
                    "error": "ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥ ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                },
            )

        # ë‚ ì§œ ë²”ìœ„ ë§ì¶”ê¸°
        common_dates = portfolio_returns.index.intersection(benchmark_returns.index)
        print(f"  Common dates: {len(common_dates)} days")

        if len(common_dates) < 20:
            return JSONResponse(
                status_code=400,
                content={
                    "error": "í¬íŠ¸í´ë¦¬ì˜¤ì™€ ë²¤ì¹˜ë§ˆí¬ì˜ ê³µí†µ ê±°ë˜ì¼ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ìµœì†Œ 20ì¼ í•„ìš”)"
                },
            )

        portfolio_returns = portfolio_returns[common_dates]
        benchmark_returns = benchmark_returns[common_dates]

        # ì„±ê³¼ ì§€í‘œ ê³„ì‚°
        print(f"\nğŸ“ˆ Calculating metrics...")
        metrics = calculate_metrics(portfolio_returns, benchmark_returns)

        # ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„
        print(f"ğŸ“Š Preparing chart data...")
        chart_data = prepare_chart_data(
            portfolio_returns, benchmark_returns, portfolio_series
        )

        # ìì‚° ë°°ë¶„ ë°ì´í„°
        print(f"ğŸ¥§ Preparing allocation data...")
        exchange_rate = get_current_exchange_rate()
        allocation_data = prepare_allocation_data(
            portfolio_data, cash_holdings, base_currency, exchange_rate
        )

        # ë³´ìœ  ì¢…ëª© í…Œì´ë¸”
        print(f"ğŸ“‹ Preparing holdings table...")
        holdings = prepare_holdings_table(
            portfolio_data, cash_holdings, base_currency, exchange_rate
        )

        print(f"\nâœ… Analysis complete!")
        print("=" * 80)

        # ê²°ê³¼ ë°˜í™˜
        return JSONResponse(
            content={
                "success": True,
                "metrics": metrics,
                "chart_data": chart_data,
                "allocation": allocation_data,
                "holdings": holdings,
                "warning": warning_msg,
            }
        )

    except Exception as e:
        error_trace = traceback.format_exc()
        print("=" * 80)
        print("âŒ ERROR in /analyze endpoint:")
        print(error_trace)
        print("=" * 80)
        return JSONResponse(
            status_code=500, content={"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
        )


@app.post("/api/ai-analysis")
async def ai_analysis(request: Request):
    """AI ë¶„ì„ API (OpenAI)"""
    try:
        # ìš”ì²­ ë°ì´í„° ì½ê¸°
        body = await request.json()
        metrics = body.get("metrics", {})
        holdings = body.get("holdings", [])

        # Rate limiting ì²´í¬ (IP ê¸°ë°˜)
        client_ip = request.client.host
        current_time = datetime.now()

        # 1ë¶„ì— 1íšŒë¡œ ì œí•œ
        if client_ip in ai_analysis_rate_limit:
            last_request = ai_analysis_rate_limit[client_ip]
            if (current_time - last_request).total_seconds() < 60:
                return JSONResponse(
                    status_code=429,
                    content={
                        "success": False,
                        "error": "ìš”ì²­ì´ ë„ˆë¬´ ë¹ ë¦…ë‹ˆë‹¤. 1ë¶„ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.",
                    },
                )

        # ìºì‹œ ì²´í¬ (IP + ë©”íŠ¸ë¦­ í•´ì‹œ)
        cache_key = f"{client_ip}_{json.dumps(metrics, sort_keys=True)}"
        if client_ip in ai_analysis_cache:
            cached = ai_analysis_cache[client_ip]
            if cached["cache_key"] == cache_key:
                # ìºì‹œ ìœ íš¨ ì‹œê°„: 1ì‹œê°„
                if (current_time - cached["timestamp"]).total_seconds() < 3600:
                    print(f"âœ… Using cached AI analysis for IP: {client_ip}")
                    return JSONResponse(
                        content={
                            "success": True,
                            "analysis": cached["result"],
                            "cached": True,
                        }
                    )

        # OpenAI API í˜¸ì¶œ
        print(f"ğŸ“¡ Calling OpenAI API for IP: {client_ip}")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ë‹¤ìŒì€ íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ì˜ ì„±ê³¼ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

**ì„±ê³¼ ì§€í‘œ:**
- ì—°í™˜ì‚° ìˆ˜ìµë¥ : {metrics.get('annual_return', 0):.2f}%
- ë³€ë™ì„±: {metrics.get('volatility', 0):.2f}%
- ìƒ¤í”„ ë¹„ìœ¨: {metrics.get('sharpe_ratio', 0):.2f}
- ìµœëŒ€ ë‚™í­: {metrics.get('max_drawdown', 0):.2f}%
- ë² íƒ€: {metrics.get('beta', 0):.2f}
- ì•ŒíŒŒ: {metrics.get('alpha', 0):.2f}%

**ë³´ìœ  ì¢…ëª© (ìƒìœ„ 5ê°œ):**
{chr(10).join([f"- {h['name']} ({h['ticker']}): {h['gain_loss_pct']:.2f}% ìˆ˜ìµë¥ " for h in holdings[:5]])}

ì´ í¬íŠ¸í´ë¦¬ì˜¤ì— ëŒ€í•´ ë‹¤ìŒì„ í¬í•¨í•˜ì—¬ ì „ë¬¸ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”:
1. ì „ë°˜ì ì¸ ì„±ê³¼ í‰ê°€ (ì¢‹ì€ ì ê³¼ ê°œì„ ì´ í•„ìš”í•œ ì )
2. ìœ„í—˜ ëŒ€ë¹„ ìˆ˜ìµ ë¶„ì„
3. í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ê°í™” ìˆ˜ì¤€
4. ê°œì„  ì œì•ˆì‚¬í•­

ë¶„ì„ì€ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì´ë©°, íˆ¬ììê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""

        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ì „ë¬¸ íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. í¬íŠ¸í´ë¦¬ì˜¤ ì„±ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
            max_tokens=2000,
        )

        analysis_result = response.choices[0].message.content

        print("âœ… OpenAI API call successful")

        # Rate limit ì—…ë°ì´íŠ¸
        ai_analysis_rate_limit[client_ip] = current_time

        # ìºì‹œ ì €ì¥
        ai_analysis_cache[client_ip] = {
            "cache_key": cache_key,
            "result": analysis_result,
            "timestamp": current_time,
        }

        return JSONResponse(
            content={"success": True, "analysis": analysis_result, "cached": False}
        )

    except Exception as e:
        error_trace = traceback.format_exc()
        print("=" * 80)
        print("âŒ ERROR in /api/ai-analysis endpoint:")
        print(error_trace)
        print("=" * 80)
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            },
        )


if __name__ == "__main__":
    import uvicorn

    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    init_database()

    # Uvicornìœ¼ë¡œ FastAPI ì•± ì‹¤í–‰
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
