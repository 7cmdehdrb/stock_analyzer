import io
import os
import sqlite3
import traceback
from datetime import datetime, timedelta
import hashlib
import hmac
import random
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

import numpy as np
import pandas as pd
import yfinance as yf
from dotenv import load_dotenv
from flask import Flask, jsonify, render_template, request, url_for, session, redirect
from flask_admin import Admin, AdminIndexView, expose
from flask_admin.contrib.sqla import ModelView
from flask_sqlalchemy import SQLAlchemy
import openai

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# Flask ë° SQLAlchemy ì„¤ì •
app = Flask(__name__)
app.config["SECRET_KEY"] = os.getenv("SECRET_KEY")
app.config["SQLALCHEMY_DATABASE_URI"] = (
    f"sqlite:///{os.path.join(os.path.dirname(__file__), 'stock_cache.db')}"
)
app.config["SQLALCHEMY_TRACK_MODIFICATIONS"] = False

# ì„¸ì…˜ ì„¤ì •
app.config["PERMANENT_SESSION_LIFETIME"] = timedelta(days=7)  # ì„¸ì…˜ ìœ íš¨ê¸°ê°„ 7ì¼

# OpenAI API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")
DEBUG = os.getenv("DEBUG", "False").lower() == "true"

# AI ë¶„ì„ rate limitingê³¼ ìºì‹±ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
ai_analysis_cache = {}  # {ip: {"timestamp": datetime, "result": dict}}
ai_analysis_rate_limit = {}  # {ip: last_request_time}

# DB íŒŒì¼ ê²½ë¡œ
DB_PATH = os.path.join(os.path.dirname(__file__), "stock_cache.db")

# í—¤ì§€í€ë“œ ë²¤ì¹˜ë§ˆí¬ ë§¤í•‘
HEDGEFUND_BENCHMARKS = {
    "HEDGEFUND_BLACKROCK": {
        "name": "ë¸”ë™ë¡ (BlackRock)",
        "csv_path": os.path.join(
            os.path.dirname(__file__), "sample_portfolio", "BLACKROCK.csv"
        ),
    },
    "HEDGEFUND_BERKSHIRE": {
        "name": "ë²„í¬ì…” í•´ì„œì›¨ì´ (Berkshire Hathaway)",
        "csv_path": os.path.join(
            os.path.dirname(__file__), "sample_portfolio", "BERKSHIRE_HATHAWAY.csv"
        ),
    },
}

# SQLAlchemy ì„¤ì •
db = SQLAlchemy(app)


# SQLAlchemy ëª¨ë¸ ì •ì˜
class StockPriceCache(db.Model):
    __tablename__ = "stock_price_cache"

    ticker = db.Column(db.String(20), primary_key=True)
    date = db.Column(db.String(10), primary_key=True)
    close_price = db.Column(db.Float, nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.now)

    def __repr__(self):
        return f"<StockPrice {self.ticker} {self.date}>"


class SavedPortfolio(db.Model):
    __tablename__ = "saved_portfolios"

    id = db.Column(db.Integer, primary_key=True, autoincrement=True)
    user_id = db.Column(
        db.Integer, db.ForeignKey("users.id"), nullable=True
    )  # ê¸°ì¡´ ë°ì´í„° í˜¸í™˜
    name = db.Column(db.String(200), nullable=False)
    csv_content = db.Column(db.Text, nullable=False)
    start_date = db.Column(db.String(10))
    benchmark_ticker = db.Column(db.String(20))
    base_currency = db.Column(db.String(3))
    created_at = db.Column(db.DateTime, default=datetime.now)
    last_accessed = db.Column(db.DateTime, default=datetime.now)

    # ê´€ê³„ ì„¤ì •
    user = db.relationship("User", backref="portfolios")

    def __repr__(self):
        return f"<Portfolio {self.name}>"


class User(db.Model):
    __tablename__ = "users"

    id = db.Column(db.Integer, primary_key=True, autoincrement=True)
    email = db.Column(db.String(255), unique=True, nullable=False)
    password_hash = db.Column(
        db.String(255), nullable=True
    )  # ì†Œì…œ ë¡œê·¸ì¸ì€ ë¹„ë°€ë²ˆí˜¸ ì—†ìŒ
    nickname = db.Column(db.String(100), unique=True, nullable=False)
    account_type = db.Column(
        db.String(20), nullable=False, default="local"
    )  # 'local', 'google', 'kakao'
    is_admin = db.Column(db.Boolean, default=False, nullable=False)  # ê´€ë¦¬ì ê¶Œí•œ
    created_at = db.Column(db.DateTime, default=datetime.now)
    last_login = db.Column(db.DateTime)
    is_active = db.Column(db.Boolean, default=True)

    def __repr__(self):
        return f"<User {self.email} ({self.account_type})>"


class EmailVerification(db.Model):
    __tablename__ = "email_verifications"

    id = db.Column(db.Integer, primary_key=True, autoincrement=True)
    email = db.Column(db.String(255), nullable=False)
    code = db.Column(db.String(6), nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.now)
    expires_at = db.Column(db.DateTime, nullable=False)
    is_verified = db.Column(db.Boolean, default=False)

    def __repr__(self):
        return f"<EmailVerification {self.email}>"


# Flask-Admin ModelView ì •ì˜
class StockPriceCacheAdmin(ModelView):
    def is_accessible(self):
        """ì ‘ê·¼ ê¶Œí•œ í™•ì¸"""
        return session.get('admin_authenticated', False)

    def inaccessible_callback(self, name, **kwargs):
        """ì ‘ê·¼ ë¶ˆê°€ëŠ¥í•  ë•Œ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
        return redirect(url_for('admin_login', next=request.url))

    column_list = ["ticker", "date", "close_price", "created_at"]
    column_searchable_list = ["ticker"]
    column_sortable_list = ["ticker", "date", "created_at"]
    column_default_sort = [("ticker", False), ("date", True)]
    page_size = 50
    can_export = True
    export_types = ["csv", "xlsx"]


class SavedPortfolioAdmin(ModelView):
    def is_accessible(self):
        """ì ‘ê·¼ ê¶Œí•œ í™•ì¸"""
        return session.get('admin_authenticated', False)

    def inaccessible_callback(self, name, **kwargs):
        """ì ‘ê·¼ ë¶ˆê°€ëŠ¥í•  ë•Œ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
        return redirect(url_for('admin_login', next=request.url))

    column_list = [
        "id",
        "user_id",
        "name",
        "start_date",
        "benchmark_ticker",
        "base_currency",
        "created_at",
        "last_accessed",
    ]
    column_searchable_list = ["name", "benchmark_ticker"]
    column_sortable_list = ["id", "name", "created_at", "last_accessed"]
    column_default_sort = [("last_accessed", True)]
    page_size = 50
    can_export = True
    export_types = ["csv", "xlsx"]
    # CSV ë‚´ìš©ì€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œì™¸ (ë„ˆë¬´ ê¸¸ì–´ì„œ)
    column_exclude_list = ["csv_content"]
    # ìƒì„¸ë³´ê¸°/ìˆ˜ì •ì—ì„œëŠ” í‘œì‹œ
    form_excluded_columns = []


class UserAdmin(ModelView):
    def is_accessible(self):
        """ì ‘ê·¼ ê¶Œí•œ í™•ì¸"""
        return session.get('admin_authenticated', False)

    def inaccessible_callback(self, name, **kwargs):
        """ì ‘ê·¼ ë¶ˆê°€ëŠ¥í•  ë•Œ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
        return redirect(url_for('admin_login', next=request.url))

    column_list = [
        "id",
        "email",
        "nickname",
        "account_type",
        "is_admin",
        "password_hash",
        "created_at",
        "last_login",
        "is_active",
    ]
    column_searchable_list = ["email", "nickname"]
    column_sortable_list = ["id", "email", "nickname", "created_at", "last_login"]
    column_default_sort = [("created_at", True)]
    column_filters = ["account_type", "is_active", "is_admin"]
    page_size = 50
    can_export = True
    export_types = ["csv", "xlsx"]
    # ëª¨ë“  ì»¬ëŸ¼ í‘œì‹œ (ìˆ¨ê¹€ ì—†ìŒ)
    form_excluded_columns = []
    # ì½ê¸° ì „ìš©ìœ¼ë¡œ í‘œì‹œ (í¬ë§¤í„° ì œê±°)
    column_formatters = {}
    # ì„¤ëª…
    column_descriptions = {
        "account_type": "local: ë¡œì»¬ ê°€ì…, google: êµ¬ê¸€, kakao: ì¹´ì¹´ì˜¤",
        "password_hash": "ì•”í˜¸í™”ëœ ë¹„ë°€ë²ˆí˜¸ (HMAC-SHA256)",
        "is_admin": "ê´€ë¦¬ì ê¶Œí•œ ì—¬ë¶€",
    }
    # ìƒì„¸ë³´ê¸°ì—ì„œ ì „ì²´ í‘œì‹œ
    column_details_list = [
        "id",
        "email",
        "nickname",
        "account_type",
        "is_admin",
        "password_hash",
        "created_at",
        "last_login",
        "is_active",
    ]


class EmailVerificationAdmin(ModelView):
    def is_accessible(self):
        """ì ‘ê·¼ ê¶Œí•œ í™•ì¸"""
        return session.get('admin_authenticated', False)

    def inaccessible_callback(self, name, **kwargs):
        """ì ‘ê·¼ ë¶ˆê°€ëŠ¥í•  ë•Œ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
        return redirect(url_for('admin_login', next=request.url))

    column_list = ["id", "email", "code", "created_at", "expires_at", "is_verified"]
    column_searchable_list = ["email", "code"]
    column_sortable_list = ["id", "email", "created_at", "expires_at"]
    column_default_sort = [("created_at", True)]
    column_filters = ["is_verified"]
    page_size = 50
    can_export = True
    export_types = ["csv", "xlsx"]


class DashboardView(AdminIndexView):
    """ì»¤ìŠ¤í…€ ëŒ€ì‹œë³´ë“œ with ë¹„ë°€ë²ˆí˜¸ ì¸ì¦"""

    def is_accessible(self):
        """ì ‘ê·¼ ê¶Œí•œ í™•ì¸ - ì„¸ì…˜ì— admin_authenticatedê°€ ìˆëŠ”ì§€ ì²´í¬"""
        return session.get('admin_authenticated', False)

    def inaccessible_callback(self, name, **kwargs):
        """ì ‘ê·¼ ë¶ˆê°€ëŠ¥í•  ë•Œ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
        return redirect(url_for('admin_login', next=request.url))

    @expose("/")
    def index(self):
        """ëŒ€ì‹œë³´ë“œ ë©”ì¸ í˜ì´ì§€"""
        # í†µê³„ ë°ì´í„° ìˆ˜ì§‘
        stats = self.get_statistics()
        recent_users = self.get_recent_users()
        recent_portfolios = self.get_recent_portfolios()
        user_type_stats = self.get_user_type_stats()

        return self.render(
            "admin/dashboard.html",
            stats=stats,
            recent_users=recent_users,
            recent_portfolios=recent_portfolios,
            user_type_stats=user_type_stats,
        )

    def get_statistics(self):
        """ì£¼ìš” í†µê³„ ë°ì´í„°"""
        try:
            # ì´ íšŒì› ìˆ˜
            total_users = User.query.count()

            # ì˜¤ëŠ˜ ê°€ì…í•œ íšŒì›
            today = datetime.now().date()
            today_users = User.query.filter(
                db.func.date(User.created_at) == today
            ).count()

            # ì´ë²ˆ ì£¼ ê°€ì…í•œ íšŒì›
            week_ago = datetime.now() - timedelta(days=7)
            week_users = User.query.filter(User.created_at >= week_ago).count()

            # ì´ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜
            total_portfolios = SavedPortfolio.query.count()

            # ì˜¤ëŠ˜ ìƒì„±ëœ í¬íŠ¸í´ë¦¬ì˜¤
            today_portfolios = SavedPortfolio.query.filter(
                db.func.date(SavedPortfolio.created_at) == today
            ).count()

            # ìºì‹œëœ ì£¼ê°€ ë°ì´í„° (í‹°ì»¤ ìˆ˜)
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(DISTINCT ticker) FROM stock_price_cache")
            cached_tickers = cursor.fetchone()[0]
            cursor.execute("SELECT COUNT(*) FROM stock_price_cache")
            cached_records = cursor.fetchone()[0]
            conn.close()

            return {
                "total_users": total_users,
                "today_users": today_users,
                "week_users": week_users,
                "total_portfolios": total_portfolios,
                "today_portfolios": today_portfolios,
                "cached_tickers": cached_tickers,
                "cached_records": cached_records,
            }
        except Exception as e:
            app.logger.error(f"í†µê³„ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {
                "total_users": 0,
                "today_users": 0,
                "week_users": 0,
                "total_portfolios": 0,
                "today_portfolios": 0,
                "cached_tickers": 0,
                "cached_records": 0,
            }

    def get_recent_users(self, limit=5):
        """ìµœê·¼ ê°€ì…í•œ íšŒì›"""
        try:
            users = User.query.order_by(User.created_at.desc()).limit(limit).all()
            return [
                {
                    "id": u.id,
                    "email": u.email,
                    "nickname": u.nickname,
                    "account_type": u.account_type,
                    "created_at": u.created_at,
                }
                for u in users
            ]
        except Exception as e:
            app.logger.error(f"ìµœê·¼ íšŒì› ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

    def get_recent_portfolios(self, limit=5):
        """ìµœê·¼ ìƒì„±ëœ í¬íŠ¸í´ë¦¬ì˜¤"""
        try:
            portfolios = (
                SavedPortfolio.query.order_by(SavedPortfolio.created_at.desc())
                .limit(limit)
                .all()
            )
            return [
                {
                    "id": p.id,
                    "name": p.name,
                    "benchmark": p.benchmark_ticker,
                    "created_at": p.created_at,
                }
                for p in portfolios
            ]
        except Exception as e:
            app.logger.error(f"ìµœê·¼ í¬íŠ¸í´ë¦¬ì˜¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

    def get_user_type_stats(self):
        """íšŒì› ìœ í˜•ë³„ í†µê³„"""
        try:
            local_count = User.query.filter_by(account_type="local").count()
            google_count = User.query.filter_by(account_type="google").count()
            kakao_count = User.query.filter_by(account_type="kakao").count()

            return {
                "local": local_count,
                "google": google_count,
                "kakao": kakao_count,
            }
        except Exception as e:
            app.logger.error(f"íšŒì› ìœ í˜• í†µê³„ ì˜¤ë¥˜: {e}")
            return {"local": 0, "google": 0, "kakao": 0}


# Flask-Admin ì´ˆê¸°í™”
admin = Admin(
    app,
    name="Portfolio Admin",
    template_mode="bootstrap3",
    index_view=DashboardView(name="Dashboard", url="/admin"),
)
admin.add_view(StockPriceCacheAdmin(StockPriceCache, db.session, name="Stock Prices"))
admin.add_view(SavedPortfolioAdmin(SavedPortfolio, db.session, name="Portfolios"))
admin.add_view(UserAdmin(User, db.session, name="Users"))
admin.add_view(
    EmailVerificationAdmin(EmailVerification, db.session, name="Email Verifications")
)


def init_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° í…Œì´ë¸” ìƒì„±"""
    # SQLAlchemyë¡œ í…Œì´ë¸” ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ)
    with app.app_context():
        db.create_all()


def hash_password(password: str) -> str:
    """ë¹„ë°€ë²ˆí˜¸ë¥¼ SHA256ìœ¼ë¡œ í•´ì‹±"""
    secret_key = app.config["SECRET_KEY"]
    if not secret_key:
        raise ValueError("SECRET_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # HMAC-SHA256 ì‚¬ìš© (ë” ì•ˆì „í•¨)
    password_bytes = password.encode("utf-8")
    key_bytes = secret_key.encode("utf-8")
    hashed = hmac.new(key_bytes, password_bytes, hashlib.sha256)
    return hashed.hexdigest()


def verify_password(password: str, password_hash: str) -> bool:
    """ë¹„ë°€ë²ˆí˜¸ ê²€ì¦"""
    return hash_password(password) == password_hash


def generate_verification_code() -> str:
    """6ìë¦¬ ì¸ì¦ë²ˆí˜¸ ìƒì„±"""
    return str(random.randint(100000, 999999))


def send_verification_email(email: str, code: str) -> bool:
    """ì´ë©”ì¼ ì¸ì¦ë²ˆí˜¸ ì „ì†¡"""
    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì´ë©”ì¼ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        smtp_server = os.getenv("SMTP_SERVER", "smtp.gmail.com")
        smtp_port = int(os.getenv("SMTP_PORT", "587"))
        sender_email = os.getenv("SENDER_EMAIL")
        sender_password = os.getenv("SENDER_PASSWORD")

        if not sender_email or not sender_password:
            app.logger.error("ì´ë©”ì¼ ì„¤ì •ì´ í™˜ê²½ë³€ìˆ˜ì— ì—†ìŠµë‹ˆë‹¤.")
            # ê°œë°œ ëª¨ë“œì—ì„œëŠ” ì½˜ì†”ì— ì¶œë ¥
            print(f"ğŸ“§ [ê°œë°œ ëª¨ë“œ] ì¸ì¦ë²ˆí˜¸: {code} (ì´ë©”ì¼: {email})")
            return True  # ê°œë°œ ëª¨ë“œì—ì„œëŠ” ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬

        # ì´ë©”ì¼ ë©”ì‹œì§€ êµ¬ì„±
        msg = MIMEMultipart()
        msg["From"] = sender_email
        msg["To"] = email
        msg["Subject"] = "í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ê¸° - ì´ë©”ì¼ ì¸ì¦ë²ˆí˜¸"

        body = f"""
        ì•ˆë…•í•˜ì„¸ìš”, í¬íŠ¸í´ë¦¬ì˜¤ ì„±ê³¼ ë¶„ì„ê¸°ì…ë‹ˆë‹¤.
        
        íšŒì›ê°€ì…ì„ ìœ„í•œ ì¸ì¦ë²ˆí˜¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
        
        ì¸ì¦ë²ˆí˜¸: {code}
        
        ì´ ì¸ì¦ë²ˆí˜¸ëŠ” 5ë¶„ê°„ ìœ íš¨í•©ë‹ˆë‹¤.
        ë³¸ì¸ì´ ìš”ì²­í•˜ì§€ ì•Šì€ ê²½ìš°, ì´ ì´ë©”ì¼ì„ ë¬´ì‹œí•´ì£¼ì„¸ìš”.
        
        ê°ì‚¬í•©ë‹ˆë‹¤.
        """

        msg.attach(MIMEText(body, "plain"))

        # SMTP ì„œë²„ ì—°ê²° ë° ì „ì†¡
        server = smtplib.SMTP(smtp_server, smtp_port)
        server.starttls()
        server.login(sender_email, sender_password)
        server.send_message(msg)
        server.quit()

        app.logger.info(f"âœ… ì¸ì¦ë²ˆí˜¸ ì´ë©”ì¼ ì „ì†¡ ì„±ê³µ: {email}")
        return True

    except Exception as e:
        app.logger.error(f"âŒ ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
        # ê°œë°œ ëª¨ë“œì—ì„œëŠ” ì½˜ì†”ì— ì¶œë ¥
        print(f"ğŸ“§ [ê°œë°œ ëª¨ë“œ] ì¸ì¦ë²ˆí˜¸: {code} (ì´ë©”ì¼: {email})")
        return True  # ê°œë°œ ëª¨ë“œì—ì„œëŠ” ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬


def get_cached_prices(ticker, start_date: pd.Timestamp, end_date: pd.Timestamp):
    """DBì—ì„œ ìºì‹œëœ ê°€ê²© ë°ì´í„° ì¡°íšŒ"""
    conn = None
    try:
        conn = sqlite3.connect(DB_PATH, timeout=30.0)
        cursor = conn.cursor()

        query = """
            SELECT date, close_price 
            FROM stock_price_cache 
            WHERE ticker = ? AND date >= ? AND date <= ?
            ORDER BY date
        """

        start_str = start_date.strftime("%Y-%m-%d")
        end_str = end_date.strftime("%Y-%m-%d")

        cursor.execute(query, (ticker, start_str, end_str))
        results = cursor.fetchall()

        if not results:
            return None

        # DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(results, columns=["date", "close_price"])
        df["date"] = pd.to_datetime(df["date"])
        df.set_index("date", inplace=True)

        return df["close_price"]
    except Exception as e:
        return None
    finally:
        if conn:
            conn.close()


def save_prices_to_cache(ticker, price_series: pd.Series):
    """ê°€ê²© ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
    if price_series is None or len(price_series) == 0:
        return

    conn = None
    try:
        conn = sqlite3.connect(DB_PATH, timeout=30.0)
        cursor = conn.cursor()

        # ë°ì´í„° ì¤€ë¹„ (NaN ê°’ ì œì™¸)
        data_to_insert = []
        for date, price in price_series.items():
            # NaN ê°’ ì²´í¬
            if pd.notna(price) and not np.isnan(price):
                date: pd.Timestamp

                date_str = date.strftime("%Y-%m-%d")
                data_to_insert.append((ticker, date_str, float(price)))

        if not data_to_insert:
            return

        # INSERT OR REPLACEë¡œ ì¤‘ë³µ ë°©ì§€
        cursor.executemany(
            """
            INSERT OR REPLACE INTO stock_price_cache (ticker, date, close_price)
            VALUES (?, ?, ?)
            """,
            data_to_insert,
        )

        conn.commit()
    except Exception as e:
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()


def get_current_exchange_rate():
    """í˜„ì¬ USD/KRW í™˜ìœ¨ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # USDKRW=X í‹°ì»¤ë¡œ í™˜ìœ¨ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        krw = yf.Ticker("USDKRW=X")
        data = krw.history(period="1d")
        if not data.empty:
            return data["Close"].iloc[-1]
        else:
            # ê¸°ë³¸ê°’ (ìµœê·¼ í‰ê·  í™˜ìœ¨)
            return 1350.0
    except Exception as e:
        return 1350.0


def fetch_stock_data(ticker, start_date, end_date):
    """Yahoo Financeì—ì„œ ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (DB ìºì‹œ í™œìš©)

    ìƒì¥ì¼ì´ start_dateë³´ë‹¤ ëŠ¦ì€ ê²½ìš°ì—ë„ ìƒì¥ ì´í›„ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ì—¬
    fill_missing_datesì—ì„œ ìƒì¥ì¼ ì´ì „ êµ¬ê°„ì„ ìƒì¥ ì‹œ ê°€ê²©ìœ¼ë¡œ ì±„ìš¸ ìˆ˜ ìˆë„ë¡ í•¨
    """
    try:
        # 1. ë¨¼ì € DB ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ
        cached_data = get_cached_prices(ticker, start_date, end_date)

        # 2. ìºì‹œì— ëª¨ë“  ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if cached_data is not None and len(cached_data) > 0:
            # ë‚ ì§œ ë²”ìœ„ í™•ì¸
            expected_start = pd.to_datetime(start_date)
            expected_end = pd.to_datetime(end_date)
            cached_start = cached_data.index.min()
            cached_end = cached_data.index.max()

            # ìºì‹œê°€ ìš”ì²­ ë²”ìœ„ë¥¼ ëª¨ë‘ ì»¤ë²„í•˜ëŠ”ì§€ í™•ì¸ (Â±7ì¼ í—ˆìš©)
            # ë˜ëŠ” ìƒì¥ì¼ì´ start_date ì´í›„ì¸ ê²½ìš°ì—ë„ ë°ì´í„° ë°˜í™˜
            if cached_start <= expected_start + timedelta(
                days=7
            ) and cached_end >= expected_end - timedelta(days=7):
                return cached_data
            elif cached_end >= expected_end - timedelta(days=7):
                # ìƒì¥ì¼ì´ ëŠ¦ì–´ë„ end_dateê¹Œì§€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°˜í™˜
                return cached_data

        # 3. ìºì‹œì— ì—†ìœ¼ë©´ Yahoo Financeì—ì„œ ê°€ì ¸ì˜¤ê¸°
        stock = yf.Ticker(ticker)
        data = stock.history(start=start_date, end=end_date)

        # ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° (ìƒì¥ì¼ì´ ëŠ¦ì„ ìˆ˜ ìˆìŒ)
        if data.empty:
            # ë” ë„“ì€ ë²”ìœ„ë¡œ ë‹¤ì‹œ ì‹œë„ (ìµœê·¼ 5ë…„ ë˜ëŠ” ìƒì¥ì¼ë¶€í„°)
            app.logger.warning(
                f"âš  No data for {ticker} in requested range, trying broader range..."
            )
            data = stock.history(period="5y")

            if data.empty:
                app.logger.warning(f"âŒ {ticker}: Still no data available")
                return None
            else:
                # ìƒì¥ì¼ ì´í›„ ë°ì´í„°ê°€ ìˆìŒ
                listing_date: pd.Timestamp = data.index.min()
                app.logger.warning(
                    f"  â„¹ {ticker} listing date appears to be around {listing_date.date()}"
                )

        price_data = data["Close"]

        # timezone ì œê±° (ìºì‹œëœ ë°ì´í„°ì™€ ì¼ê´€ì„± ìœ ì§€)
        if hasattr(price_data.index, "tz") and price_data.index.tz is not None:
            price_data.index = price_data.index.tz_localize(None)

        # NaN ê°’ ì œê±°
        price_data = price_data.dropna()

        if len(price_data) == 0:
            app.logger.warning(f"âŒ {ticker}: No valid price data after cleaning")
            return None

        # 4. ìƒˆë¡œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ DBì— ì €ì¥
        save_prices_to_cache(ticker, price_data)

        return price_data

    except Exception as e:
        app.logger.warning(f"âŒ Error fetching {ticker}: {e}")
        return None


def normalize_ticker(ticker, country):
    """í‹°ì»¤ ì •ê·œí™” (í•œêµ­ ì¢…ëª©ì— .KS ìë™ ì¶”ê°€)"""
    ticker = str(ticker).strip()

    # í•œêµ­ ì¢…ëª©ì¸ ê²½ìš°
    if country == "í•œêµ­":
        # ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì§„ ê²½ìš° (ì˜ˆ: 005930)
        if ticker.isdigit():
            ticker = f"{ticker}.KS"
            return ticker
        # ì´ë¯¸ .KSë‚˜ .KQê°€ ë¶™ì–´ìˆì§€ ì•Šì€ ê²½ìš°
        elif not (ticker.endswith(".KS") or ticker.endswith(".KQ")):
            ticker = f"{ticker}.KS"
            return ticker

    return ticker


def merge_duplicate_tickers(portfolio_df: pd.DataFrame) -> pd.DataFrame:
    """ë™ì¼ í‹°ì»¤ë¥¼ ê°€ì§„ ì¢…ëª©ì˜ ë³´ìœ ëŸ‰ì„ í•©ì‚°"""
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    if "í‹°ì»¤" not in portfolio_df.columns or "ë³´ìœ ëŸ‰" not in portfolio_df.columns:
        return portfolio_df

    # í‹°ì»¤ ì •ê·œí™” (í•œêµ­ ì¢…ëª©ì— .KS ì¶”ê°€)
    if "êµ­ê°€" in portfolio_df.columns:
        portfolio_df["í‹°ì»¤"] = portfolio_df.apply(
            lambda row: normalize_ticker(row["í‹°ì»¤"], row.get("êµ­ê°€", "")), axis=1
        )

    # í‹°ì»¤ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë³´ìœ ëŸ‰ í•©ì‚°
    # ì²« ë²ˆì§¸ í–‰ì˜ ë‹¤ë¥¸ ì •ë³´(ì¢…ëª©ëª…, êµ­ê°€, ë¶„ë¥˜ ë“±)ëŠ” ìœ ì§€
    grouped = portfolio_df.groupby("í‹°ì»¤", as_index=False).agg(
        {
            "ë³´ìœ ëŸ‰": "sum",  # ë³´ìœ ëŸ‰ì€ í•©ì‚°
            **{
                col: "first"
                for col in portfolio_df.columns
                if col not in ["í‹°ì»¤", "ë³´ìœ ëŸ‰"]
            },
        }
    )

    return grouped


def fill_missing_dates(price_series, start_date, end_date):
    """íœ´ì¥ì¼ ë° ìƒì¥ì¼ë¡œ ì¸í•œ ë¹ˆ ë°ì´í„° ì²˜ë¦¬

    Args:
        price_series: ì£¼ê°€ ì‹œê³„ì—´ ë°ì´í„°
        start_date: ë¶„ì„ ì‹œì‘ ë‚ ì§œ
        end_date: ë¶„ì„ ì¢…ë£Œ ë‚ ì§œ

    Returns:
        ë³´ê°„ëœ ì£¼ê°€ ì‹œê³„ì—´ ë°ì´í„°
    """
    if price_series is None or len(price_series) == 0:
        print("  âš  fill_missing_dates: No data provided")
        return None

    try:
        # ì›ë³¸ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ë¥¼ timezone-naiveë¡œ ë³€í™˜
        if hasattr(price_series.index, "tz") and price_series.index.tz is not None:
            price_series.index = price_series.index.tz_localize(None)

        # ì¸ë±ìŠ¤ê°€ DatetimeIndexì¸ì§€ í™•ì¸
        if not isinstance(price_series.index, pd.DatetimeIndex):
            price_series.index = pd.to_datetime(price_series.index)

        # ì „ì²´ ë‚ ì§œ ë²”ìœ„ ìƒì„± (ëª¨ë“  ë‚ ì§œ í¬í•¨)
        all_dates = pd.date_range(start=start_date, end=end_date, freq="D")

        # ê¸°ì¡´ ë°ì´í„°ë¥¼ ì „ì²´ ë‚ ì§œ ë²”ìœ„ë¡œ í™•ì¥
        price_series_filled = price_series.reindex(all_dates)

        # ë°ì´í„°ê°€ ìˆëŠ” ì²« ë‚ ì§œ í™•ì¸ (ìƒì¥ì¼)
        first_valid_date = price_series_filled.first_valid_index()

        if first_valid_date is None:
            # ê·¸ëƒ¥ ì›ë³¸ ë°ì´í„° ë°˜í™˜ (ë‚ ì§œ í™•ì¥ ì—†ì´)
            return price_series

        # ìƒì¥ ì´ì „ ë°ì´í„°: ìƒì¥ í›„ ì²« ê°€ê²©ìœ¼ë¡œ ì±„ì›€
        first_price = price_series_filled[first_valid_date]
        price_series_filled.loc[:first_valid_date] = first_price

        # ìƒì¥ ì´í›„ ë¹ˆ ë°ì´í„°: ì„ í˜• ë³´ê°„
        price_series_filled = price_series_filled.interpolate(
            method="linear", limit_direction="forward"
        )

        # ì•„ì§ë„ ë¹ˆ ê°’ì´ ìˆë‹¤ë©´ (ë ë¶€ë¶„) forward fill
        price_series_filled = price_series_filled.ffill()

        # ê·¸ë˜ë„ ë‚¨ì•„ìˆëŠ” NaNì€ backward fill
        price_series_filled = price_series_filled.bfill()

        filled_count = len(all_dates) - len(price_series)

        return price_series_filled

    except Exception as e:
        app.logger.warning(f"âŒ Error in fill_missing_dates: {e}")
        import traceback

        traceback.print_exc()
        # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë°ì´í„° ë°˜í™˜
        return price_series


def calculate_portfolio_returns(
    portfolio_df: pd.DataFrame, start_date: pd.Timestamp, base_currency="USD"
):
    """í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê³„ì‚° (ê¸°ì¤€ í†µí™” ì ìš©, í˜„ê¸ˆ ì œì™¸)"""
    end_date = datetime.now()

    # í˜„ì¬ í™˜ìœ¨ ê°€ì ¸ì˜¤ê¸°
    exchange_rate = get_current_exchange_rate()
    print(f"Current USD/KRW exchange rate: {exchange_rate}")

    # ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥  ë°ì´í„° ìˆ˜ì§‘
    portfolio_data = {}
    cash_holdings = {}
    failed_tickers = []  # ì‹¤íŒ¨í•œ í‹°ì»¤ ì¶”ì 
    total_initial_value = 0
    total_initial_value_with_cash = 0

    for _, row in portfolio_df.iterrows():
        ticker = row["í‹°ì»¤"]
        quantity = row["ë³´ìœ ëŸ‰"]
        country = row.get("êµ­ê°€", "ë¯¸êµ­")  # ê¸°ë³¸ê°’ì€ ë¯¸êµ­
        asset_class = row.get("ë¶„ë¥˜", "")

        # í˜„ê¸ˆì¸ ê²½ìš° ë³„ë„ ì²˜ë¦¬
        if asset_class == "í˜„ê¸ˆ":
            # í˜„ê¸ˆ ê°€ì¹˜ ê³„ì‚° (í™˜ìœ¨ ì ìš©)
            if country == "í•œêµ­" and base_currency == "USD":
                cash_value = quantity / exchange_rate
            elif country == "ë¯¸êµ­" and base_currency == "KRW":
                cash_value = quantity * exchange_rate
            else:
                cash_value = quantity

            cash_holdings[ticker] = {
                "value": cash_value,
                "country": country,
                "ticker": ticker,
            }
            total_initial_value_with_cash += cash_value
            continue

        # ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        price_data = fetch_stock_data(ticker, start_date, end_date)

        if price_data is None or len(price_data) == 0:
            app.logger.warning(f"âš  Skipping {ticker}: No price data available")
            failed_tickers.append(ticker)  # ì‹¤íŒ¨í•œ í‹°ì»¤ ê¸°ë¡
            continue

        # fill_missing_datesë¥¼ í˜¸ì¶œí•˜ì—¬ ìƒì¥ì¼ ì´ì „ ë°ì´í„°ë¥¼ ìƒì¥ ì‹œ ê°€ê²©ìœ¼ë¡œ ì±„ì›€
        app.logger.info(f"ğŸ”„ Filling missing dates for {ticker}...")
        price_data = fill_missing_dates(price_data, start_date, end_date)

        if price_data is None or len(price_data) == 0:
            app.logger.warning(f"âš  Skipping {ticker}: Failed to process price data")
            failed_tickers.append(ticker)
            continue

        # í™˜ìœ¨ ì ìš©
        # ê¸°ì¤€ í†µí™”ê°€ USDì´ê³  í•œêµ­ ì£¼ì‹ì¸ ê²½ìš° -> USDë¡œ í™˜ì‚°
        # ê¸°ì¤€ í†µí™”ê°€ KRWì´ê³  ë¯¸êµ­ ì£¼ì‹ì¸ ê²½ìš° -> KRWë¡œ í™˜ì‚°
        if base_currency == "USD" and country == "í•œêµ­":
            # í•œêµ­ ì£¼ì‹ì„ USDë¡œ í™˜ì‚° (KRW / í™˜ìœ¨)
            price_data = price_data / exchange_rate
        elif base_currency == "KRW" and country == "ë¯¸êµ­":
            # ë¯¸êµ­ ì£¼ì‹ì„ KRWë¡œ í™˜ì‚° (USD * í™˜ìœ¨)
            price_data = price_data * exchange_rate

        initial_price = price_data.iloc[0]
        initial_value = initial_price * quantity
        total_initial_value += initial_value
        total_initial_value_with_cash += initial_value

        portfolio_data[ticker] = {
            "prices": price_data,
            "quantity": quantity,
            "initial_value": initial_value,
            "country": country,
            "asset_class": asset_class,
            "name": row.get("ì¢…ëª©ëª…", ticker),
        }

    if not portfolio_data:
        app.logger.warning(
            f"âŒ No valid portfolio data found. Cash holdings: {len(cash_holdings)}"
        )
        return (
            None,
            None,
            None,
            cash_holdings,
            total_initial_value_with_cash,
            failed_tickers,
        )

    # ëª¨ë“  ë‚ ì§œì˜ í•©ì§‘í•© êµ¬í•˜ê¸° (start_date ì´í›„ë§Œ)
    all_dates = pd.DatetimeIndex([])

    # start_dateë¥¼ timezone-naiveë¡œ ë³€í™˜ (fetch_stock_dataê°€ timezone ì—†ëŠ” ë°ì´í„° ë°˜í™˜)
    start_date_tz = pd.to_datetime(start_date)

    for data in portfolio_data.values():
        prices_index = data["prices"].index

        # start_date ì´í›„ ë°ì´í„°ë§Œ ì‚¬ìš©
        ticker_dates = prices_index[prices_index >= start_date_tz]
        all_dates = all_dates.union(ticker_dates)

    all_dates = sorted(all_dates)

    # í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ ê°€ì¹˜ ê³„ì‚°
    portfolio_values = []

    for date in all_dates:
        daily_value = 0
        for ticker, data in portfolio_data.items():
            # í•´ë‹¹ ë‚ ì§œì˜ ê°€ê²© (ì—†ìœ¼ë©´ forward fill)
            if date in data["prices"].index:
                price = data["prices"][date]
            else:
                # ê°€ì¥ ìµœê·¼ ê°€ê²© ì‚¬ìš©
                available_prices = data["prices"][data["prices"].index <= date]
                if len(available_prices) > 0:
                    price = available_prices.iloc[-1]
                else:
                    price = data["prices"].iloc[0]

            daily_value += price * data["quantity"]

        portfolio_values.append(daily_value)

    portfolio_series = pd.Series(portfolio_values, index=all_dates)

    # ì¼ì¼ ìˆ˜ìµë¥  ê³„ì‚°
    returns = portfolio_series.pct_change().dropna()

    # í¬íŠ¸í´ë¦¬ì˜¤ ë°ì´í„°ì™€ í˜„ê¸ˆ ë³´ìœ  ì •ë³´ ë°˜í™˜
    return (
        returns,
        portfolio_series,
        portfolio_data,
        cash_holdings,
        total_initial_value_with_cash,
        failed_tickers,  # ì‹¤íŒ¨í•œ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    )


def calculate_weighted_annual_return(portfolio_returns: dict):
    """ì—° í‰ê·  ìˆ˜ìµë¥  ê³„ì‚° (ì˜ì—…ì¼ ê°€ì¤‘í‰ê· )"""
    if len(portfolio_returns) == 0:
        return 0

    # ë‚ ì§œë¥¼ ì—°ë„ë³„ë¡œ ê·¸ë£¹í™”
    returns_by_year: dict[int, list[float]] = {}

    for date, ret in portfolio_returns.items():
        year = date.year
        if year not in returns_by_year:
            returns_by_year[year] = []
        returns_by_year[year].append(ret)

    # ê° ì—°ë„ì˜ ìˆ˜ìµë¥ ê³¼ ì˜ì—…ì¼ ìˆ˜ ê³„ì‚°
    yearly_data = []
    for year, returns_list in returns_by_year.items():
        trading_days = len(returns_list)
        # í•´ë‹¹ ì—°ë„ì˜ ëˆ„ì  ìˆ˜ìµë¥ 
        year_cumulative = (1 + pd.Series(returns_list)).prod() - 1
        # ì—°ìœ¨í™” (í•´ë‹¹ ì—°ë„ì˜ ì¼ë¶€ë§Œ ìˆëŠ” ê²½ìš° ë³´ì •)
        year_return = (
            ((1 + year_cumulative) ** (252 / trading_days) - 1)
            if trading_days > 0
            else 0
        )
        yearly_data.append(
            {"year": year, "return": year_return, "trading_days": trading_days}
        )

    # ì˜ì—…ì¼ ê°€ì¤‘ í‰ê· 
    total_trading_days = sum(d["trading_days"] for d in yearly_data)
    if total_trading_days == 0:
        return 0

    weighted_return = (
        sum(d["return"] * d["trading_days"] for d in yearly_data) / total_trading_days
    )

    # for d in yearly_data:
    #     weight = d["trading_days"] / total_trading_days * 100

    return weighted_return


def calculate_hedgefund_benchmark_returns(
    hedgefund_key: str, start_date: pd.Timestamp, base_currency: str
):
    """í—¤ì§€í€ë“œ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë²¤ì¹˜ë§ˆí¬ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ìˆ˜ìµë¥  ê³„ì‚°

    Args:
        hedgefund_key: HEDGEFUND_BLACKROCK ë˜ëŠ” HEDGEFUND_BERKSHIRE ë“±
        start_date: ë¶„ì„ ì‹œì‘ ë‚ ì§œ
        base_currency: ê¸°ì¤€ í†µí™” (USD ë˜ëŠ” KRW)

    Returns:
        pd.Series: í—¤ì§€í€ë“œ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ì¼ì¼ ìˆ˜ìµë¥ 
    """
    if hedgefund_key not in HEDGEFUND_BENCHMARKS:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” í—¤ì§€í€ë“œ ë²¤ì¹˜ë§ˆí¬: {hedgefund_key}")

    hedgefund_info = HEDGEFUND_BENCHMARKS[hedgefund_key]
    csv_path = hedgefund_info["csv_path"]

    app.logger.info(f"ğŸ“Š í—¤ì§€í€ë“œ ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ: {hedgefund_info['name']}")
    app.logger.info(f"ğŸ“ CSV ê²½ë¡œ: {csv_path}")

    # CSV íŒŒì¼ ì½ê¸°
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"í—¤ì§€í€ë“œ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

    hedgefund_df = pd.read_csv(csv_path)

    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_columns = ["í‹°ì»¤", "ë³´ìœ ëŸ‰", "êµ­ê°€", "ë¶„ë¥˜"]
    missing_columns = [
        col for col in required_columns if col not in hedgefund_df.columns
    ]

    if missing_columns:
        raise ValueError(
            f'í—¤ì§€í€ë“œ CSV íŒŒì¼ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤: {", ".join(missing_columns)}'
        )

    # ë™ì¼ í‹°ì»¤ ë³‘í•©
    hedgefund_df = merge_duplicate_tickers(hedgefund_df)

    # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê³„ì‚°
    result = calculate_portfolio_returns(hedgefund_df, start_date, base_currency)

    if result is None or result[0] is None:
        failed_tickers = result[5] if result and len(result) > 5 else []
        error_msg = f"í—¤ì§€í€ë“œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        if failed_tickers:
            error_msg += f" ì‹¤íŒ¨í•œ í‹°ì»¤: {', '.join(failed_tickers)}"
        raise ValueError(error_msg)

    hedgefund_returns = result[0]  # ì¼ì¼ ìˆ˜ìµë¥ 

    app.logger.info(
        f"âœ… í—¤ì§€í€ë“œ ë²¤ì¹˜ë§ˆí¬ ê³„ì‚° ì™„ë£Œ: {len(hedgefund_returns)} ë°ì´í„° í¬ì¸íŠ¸"
    )

    return hedgefund_returns


def calculate_metrics(portfolio_returns: pd.Series, benchmark_returns: pd.Series):
    """ìƒ¤í”„ë¹„, ì†Œí‹°ë…¸ë¹„, ì•ŒíŒŒ, ë² íƒ€, í‰ê·  ì—° ìˆ˜ìµë¥  ê³„ì‚°"""

    # ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥
    app.logger.info(f"\nğŸ” Index comparison:")
    app.logger.info(f"Portfolio index type: {type(portfolio_returns.index)}")
    app.logger.info(
        f"Portfolio index tz: {getattr(portfolio_returns.index, 'tz', 'N/A')}"
    )
    app.logger.info(f"Portfolio first date: {portfolio_returns.index[0]}")
    app.logger.info(f"Benchmark index type: {type(benchmark_returns.index)}")
    app.logger.info(
        f"Benchmark index tz: {getattr(benchmark_returns.index, 'tz', 'N/A')}"
    )
    app.logger.info(f"Benchmark first date: {benchmark_returns.index[0]}")

    # timezone í†µì¼
    if (
        hasattr(portfolio_returns.index, "tz")
        and portfolio_returns.index.tz is not None
    ):
        portfolio_returns.index = portfolio_returns.index.tz_localize(None)

    if (
        hasattr(benchmark_returns.index, "tz")
        and benchmark_returns.index.tz is not None
    ):
        benchmark_returns.index = benchmark_returns.index.tz_localize(None)

    # ê³µí†µ ë‚ ì§œë§Œ ì‚¬ìš©
    common_dates = portfolio_returns.index.intersection(benchmark_returns.index)

    portfolio_returns = portfolio_returns[common_dates]
    benchmark_returns = benchmark_returns[common_dates]

    if len(portfolio_returns) == 0:
        app.logger.warning(f"âŒ No common dates found!")
        return None

    # ìˆ˜ìµë¥  ì°¨ì´ í™•ì¸
    # returns_diff = (portfolio_returns - benchmark_returns).abs().mean()

    # ìƒ˜í”Œ ë¹„êµ
    # for i in range(min(5, len(common_dates))):
    #     date = common_dates[i]

    # ì—°ê°„í™” ê³„ì‚°ì„ ìœ„í•œ ê±°ë˜ì¼ ìˆ˜
    trading_days = 252

    # í‰ê·  ì—° ìˆ˜ìµë¥ 
    avg_return = portfolio_returns.mean() * trading_days

    # í‘œì¤€í¸ì°¨ (ì—°ê°„í™”)
    std_dev = portfolio_returns.std() * np.sqrt(trading_days)

    # ìƒ¤í”„ ë¹„ìœ¨ (ë¬´ìœ„í—˜ ìˆ˜ìµë¥  0ìœ¼ë¡œ ê°€ì •)
    sharpe_ratio = avg_return / std_dev if std_dev != 0 else 0

    # app.logger.info(f"\n  ğŸ“ˆ Sharpe Calculation:")
    # app.logger.info(f"Annualized return: {avg_return * 100:.2f}%")
    # app.logger.info(f"Annualized volatility: {std_dev * 100:.2f}%")
    # app.logger.info(f"Sharpe ratio: {sharpe_ratio:.4f}")

    # ì†Œí‹°ë…¸ ë¹„ìœ¨ (í•˜ë°© í‘œì¤€í¸ì°¨)
    downside_returns = portfolio_returns[portfolio_returns < 0]
    downside_std = downside_returns.std() * np.sqrt(trading_days)
    sortino_ratio = avg_return / downside_std if downside_std != 0 else 0

    # app.logger.info(f"\n  ğŸ“‰ Sortino Calculation:")
    # app.logger.info(
    #     f"    Downside returns count: {len(downside_returns)}/{len(portfolio_returns)}"
    # )
    # app.logger.info(f"Downside volatility: {downside_std * 100:.2f}%")
    # app.logger.info(f"Sortino ratio: {sortino_ratio:.4f}")

    # ë² íƒ€ ê³„ì‚° ìˆ˜ì • - ê³µë¶„ì‚°ê³¼ ë¶„ì‚° ëª¨ë‘ ì¼ì¼ ìˆ˜ìµë¥  ê¸°ì¤€
    covariance = portfolio_returns.cov(benchmark_returns)
    benchmark_variance = benchmark_returns.var()
    beta = covariance / benchmark_variance if benchmark_variance != 0 else 0

    # app.logger.info(f"Covariance: {covariance:.6f}")
    # app.logger.info(f"Benchmark variance: {benchmark_variance:.6f}")
    # app.logger.info(f"Beta: {beta:.4f}")

    # ì•ŒíŒŒ (ì—°ê°„í™”)
    benchmark_avg_return = benchmark_returns.mean() * trading_days
    alpha = avg_return - (beta * benchmark_avg_return)

    # app.logger.info(f"Portfolio annual return: {avg_return * 100:.2f}%")
    # app.logger.info(f"Benchmark annual return: {benchmark_avg_return * 100:.2f}%")
    # app.logger.info(f"Alpha: {alpha * 100:.2f}%")

    # ëˆ„ì  ìˆ˜ìµë¥ 
    cumulative_return = (1 + portfolio_returns).prod() - 1

    # ì—°ìˆ˜ ê³„ì‚°
    years = len(portfolio_returns) / trading_days

    # ì—°í‰ê·  ìˆ˜ìµë¥  (CAGR)
    if years > 0:
        cagr = (1 + cumulative_return) ** (1 / years) - 1
    else:
        cagr = 0

    # ë²¤ì¹˜ë§ˆí¬ CAGR ê³„ì‚°
    benchmark_cumulative_return = (1 + benchmark_returns).prod() - 1
    if years > 0:
        benchmark_cagr = (1 + benchmark_cumulative_return) ** (1 / years) - 1
    else:
        benchmark_cagr = 0

    # ë²¤ì¹˜ë§ˆí¬ ìƒ¤í”„/ì†Œí‹°ë…¸ ê³„ì‚°
    benchmark_std = benchmark_returns.std() * np.sqrt(trading_days)
    benchmark_sharpe = benchmark_avg_return / benchmark_std if benchmark_std != 0 else 0

    benchmark_downside_returns = benchmark_returns[benchmark_returns < 0]
    benchmark_downside_std = benchmark_downside_returns.std() * np.sqrt(trading_days)
    benchmark_sortino = (
        benchmark_avg_return / benchmark_downside_std
        if benchmark_downside_std != 0
        else 0
    )

    # print(f"\n  ğŸ“Š Benchmark metrics:")
    # print(f"    Sharpe: {benchmark_sharpe:.4f}")
    # print(f"    Sortino: {benchmark_sortino:.4f}")
    # print(f"    CAGR: {benchmark_cagr * 100:.2f}%")

    # ì—° í‰ê·  ìˆ˜ìµë¥  (ì˜ì—…ì¼ ê°€ì¤‘í‰ê· ) ê³„ì‚°
    weighted_annual_return = calculate_weighted_annual_return(portfolio_returns)

    metrics = {
        "sharpe_ratio": round(sharpe_ratio, 4),
        "sortino_ratio": round(sortino_ratio, 4),
        "benchmark_sharpe_ratio": round(benchmark_sharpe, 4),
        "benchmark_sortino_ratio": round(benchmark_sortino, 4),
        "alpha": round(alpha * 100, 2),  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
        "beta": round(beta, 4),
        "annual_return": round(avg_return * 100, 2),  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
        "weighted_annual_return": round(
            weighted_annual_return * 100, 2
        ),  # ì—° í‰ê·  ìˆ˜ìµë¥ 
        "benchmark_annual_return": round(benchmark_cagr * 100, 2),  # ë²¤ì¹˜ë§ˆí¬ CAGR
        "cagr": round(cagr * 100, 2),  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
        "cumulative_return": round(cumulative_return * 100, 2),
        "volatility": round(std_dev * 100, 2),
        "benchmark_volatility": round(benchmark_std * 100, 2),
    }

    return metrics


def prepare_chart_data(portfolio_returns, benchmark_returns, portfolio_series):
    """ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„"""
    # timezone í†µì¼ (calculate_metricsì™€ ë™ì¼)
    if (
        hasattr(portfolio_returns.index, "tz")
        and portfolio_returns.index.tz is not None
    ):
        portfolio_returns.index = portfolio_returns.index.tz_localize(None)

    if (
        hasattr(benchmark_returns.index, "tz")
        and benchmark_returns.index.tz is not None
    ):
        benchmark_returns.index = benchmark_returns.index.tz_localize(None)

    if hasattr(portfolio_series.index, "tz") and portfolio_series.index.tz is not None:
        portfolio_series.index = portfolio_series.index.tz_localize(None)

    common_dates = portfolio_returns.index.intersection(benchmark_returns.index)

    # ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°
    portfolio_cumulative = (1 + portfolio_returns[common_dates]).cumprod()
    benchmark_cumulative = (1 + benchmark_returns[common_dates]).cumprod()

    # ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    dates = [date.strftime("%Y-%m-%d") for date in common_dates]

    chart_data = {
        "dates": dates,
        "portfolio_cumulative": [
            round(val, 4) for val in portfolio_cumulative.tolist()
        ],
        "benchmark_cumulative": [
            round(val, 4) for val in benchmark_cumulative.tolist()
        ],
        "portfolio_values": [
            round(val, 2) for val in portfolio_series[common_dates].tolist()
        ],
    }

    return chart_data


def prepare_allocation_data(
    portfolio_data, cash_holdings, total_initial_value_with_cash
):
    """ë„ë„› ì°¨íŠ¸ìš© ìì‚° ë°°ë¶„ ë°ì´í„° ì¤€ë¹„"""

    # ì‹œì‘ ì‹œì  ë°°ë¶„
    initial_allocation = {}
    for ticker, data in portfolio_data.items():
        name = data.get("name", ticker)
        initial_allocation[name] = data["initial_value"]

    # í˜„ê¸ˆ ì¶”ê°€
    for ticker, cash_data in cash_holdings.items():
        name = f"í˜„ê¸ˆ ({cash_data['country']})"
        if name in initial_allocation:
            initial_allocation[name] += cash_data["value"]
        else:
            initial_allocation[name] = cash_data["value"]

    # í˜„ì¬ ì‹œì  ë°°ë¶„
    current_allocation = {}
    for ticker, data in portfolio_data.items():
        name = data.get("name", ticker)
        current_price = data["prices"].iloc[-1]
        current_value = current_price * data["quantity"]
        current_allocation[name] = current_value

    # í˜„ê¸ˆ ì¶”ê°€ (í˜„ê¸ˆì€ ê°€ì¹˜ ë³€ë™ ì—†ìŒ)
    for ticker, cash_data in cash_holdings.items():
        name = f"í˜„ê¸ˆ ({cash_data['country']})"
        if name in current_allocation:
            current_allocation[name] += cash_data["value"]
        else:
            current_allocation[name] = cash_data["value"]

    # ìƒìœ„ 10ê°œ ì¢…ëª©ë§Œ í‘œì‹œ, ë‚˜ë¨¸ì§€ëŠ” "ê¸°íƒ€"ë¡œ ë¬¶ê¸°
    def get_top_allocations(allocation_dict, top_n=10):
        sorted_items = sorted(allocation_dict.items(), key=lambda x: x[1], reverse=True)

        if len(sorted_items) <= top_n:
            return dict(sorted_items)

        top_items = dict(sorted_items[:top_n])
        others_sum = sum(value for _, value in sorted_items[top_n:])
        if others_sum > 0:
            top_items["ê¸°íƒ€"] = others_sum

        return top_items

    initial_top = get_top_allocations(initial_allocation)
    current_top = get_top_allocations(current_allocation)

    return {
        "initial": {
            "labels": list(initial_top.keys()),
            "values": [round(v, 2) for v in initial_top.values()],
        },
        "current": {
            "labels": list(current_top.keys()),
            "values": [round(v, 2) for v in current_top.values()],
        },
    }


def prepare_holdings_table(
    portfolio_data: dict, cash_holdings: dict, base_currency: str, exchange_rate: float
):
    """ë³´ìœ  ì¢…ëª© í…Œì´ë¸” ë°ì´í„° ì¤€ë¹„ (í˜„ì¬ ì‹œì  ê¸°ì¤€)"""
    holdings = []

    # íˆ¬ì ìì‚°
    for ticker, data in portfolio_data.items():
        data: dict
        current_price = data["prices"].iloc[-1]
        quantity = data["quantity"]
        current_value = current_price * quantity
        name = data.get("name", ticker)

        holdings.append(
            {
                "ticker": ticker,
                "name": name,
                "quantity": quantity,
                "current_value": round(current_value, 2),
                "asset_class": data.get("asset_class", ""),
            }
        )

    # í˜„ê¸ˆ
    for ticker, cash_data in cash_holdings.items():
        holdings.append(
            {
                "ticker": ticker,
                "name": f"í˜„ê¸ˆ ({cash_data['country']})",
                "quantity": cash_data["value"],
                "current_value": round(cash_data["value"], 2),
                "asset_class": "í˜„ê¸ˆ",
            }
        )

    # ì´ ê°€ì¹˜ ê³„ì‚°
    total_value = sum(h["current_value"] for h in holdings)

    # ë¹„ì¤‘ ê³„ì‚°
    for holding in holdings:
        holding["weight"] = (
            round((holding["current_value"] / total_value * 100), 2)
            if total_value > 0
            else 0
        )

    # í˜„ì¬ ê°€ì¹˜ ê¸°ì¤€ ì •ë ¬
    holdings.sort(key=lambda x: x["current_value"], reverse=True)

    return holdings


@app.route("/")
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template("index.html")


@app.route("/api/cache-stats", methods=["GET"])
def get_cache_stats():
    """ìºì‹œ í†µê³„ ì¡°íšŒ"""
    conn = None
    try:
        conn = sqlite3.connect(DB_PATH, timeout=30.0)
        cursor = conn.cursor()

        # ìºì‹œëœ í‹°ì»¤ ìˆ˜ì™€ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ
        cursor.execute(
            """
            SELECT COUNT(DISTINCT ticker) as ticker_count,
                   COUNT(*) as record_count
            FROM stock_price_cache
        """
        )

        result = cursor.fetchone()
        ticker_count = result[0] if result else 0
        record_count = result[1] if result else 0

        return jsonify({"ticker_count": ticker_count, "record_count": record_count})
    except Exception as e:
        print(f"Error getting cache stats: {e}")
        return jsonify({"error": str(e)}), 500
    finally:
        if conn:
            conn.close()


@app.route("/api/save-portfolio", methods=["POST"])
def save_portfolio():
    """í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    try:
        # ë¡œê·¸ì¸ ì²´í¬ - í•„ìˆ˜
        if "user_id" not in session:
            return jsonify({"error": "ë¡œê·¸ì¸ì´ í•„ìš”í•œ ê¸°ëŠ¥ì…ë‹ˆë‹¤."}), 401

        user_id = session["user_id"]

        data = request.json

        # í•„ìˆ˜ ë°ì´í„° í™•ì¸
        required_fields = [
            "name",
            "csv_content",
            "start_date",
            "benchmark_ticker",
            "base_currency",
            "metrics",
            "summary",
            "holdings_table",
            "allocation_data",
            "chart_data",
        ]

        missing_fields = [field for field in required_fields if field not in data]
        if missing_fields:
            return (
                jsonify({"error": f"í•„ìˆ˜ ë°ì´í„° ëˆ„ë½: {', '.join(missing_fields)}"}),
                400,
            )

        # JSONìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        import json

        portfolio = SavedPortfolio(
            user_id=user_id,
            name=data["name"],
            csv_content=data["csv_content"],
            start_date=data["start_date"],
            benchmark_ticker=data["benchmark_ticker"],
            base_currency=data["base_currency"],
            created_at=datetime.now(),
            last_accessed=datetime.now(),
        )

        # ë¶„ì„ ê²°ê³¼ë¥¼ csv_contentì— JSONìœ¼ë¡œ ì¶”ê°€ ì €ì¥
        full_data = {
            "csv_content": data["csv_content"],
            "metrics": data["metrics"],
            "summary": data["summary"],
            "holdings_table": data["holdings_table"],
            "allocation_data": data["allocation_data"],
            "chart_data": data["chart_data"],
        }
        portfolio.csv_content = json.dumps(full_data, ensure_ascii=False)

        db.session.add(portfolio)
        db.session.commit()

        return jsonify(
            {
                "success": True,
                "portfolio_id": portfolio.id,
                "message": f"í¬íŠ¸í´ë¦¬ì˜¤ '{data['name']}'ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
            }
        )

    except Exception as e:
        db.session.rollback()
        error_trace = traceback.format_exc()
        print("=" * 80)
        print("âŒ ERROR in /api/save-portfolio endpoint:")
        print(error_trace)
        print("=" * 80)
        return jsonify({"error": f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500


@app.route("/ranking")
def ranking():
    """ë­í‚¹ í˜ì´ì§€"""
    return render_template("ranking.html")


@app.route("/login")
def login():
    """ë¡œê·¸ì¸ í˜ì´ì§€"""
    # ì´ë¯¸ ë¡œê·¸ì¸ëœ ê²½ìš° ë©”ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    if "user_id" in session:
        return redirect("/")
    return render_template("login.html")


@app.route("/signup")
def signup():
    """íšŒì›ê°€ì… í˜ì´ì§€"""
    return render_template("signup.html")


@app.route("/mypage")
def mypage():
    """ë§ˆì´í˜ì´ì§€"""
    # ë¡œê·¸ì¸ í™•ì¸
    if "user_id" not in session:
        return redirect("/login")
    return render_template("mypage.html")


@app.route("/admin-login", methods=["GET", "POST"])
def admin_login():
    """ê´€ë¦¬ì ë¡œê·¸ì¸ í˜ì´ì§€"""
    if request.method == "POST":
        password = request.form.get("password")
        admin_password = os.getenv("ADMIN_PW")
        
        if not admin_password:
            return render_template("admin_login.html", error="ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if password == admin_password:
            session["admin_authenticated"] = True
            session.permanent = True  # ì„¸ì…˜ ìœ ì§€
            next_url = request.args.get("next", "/admin")
            return redirect(next_url)
        else:
            return render_template("admin_login.html", error="ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    return render_template("admin_login.html")


@app.route("/admin-logout")
def admin_logout():
    """ê´€ë¦¬ì ë¡œê·¸ì•„ì›ƒ"""
    session.pop("admin_authenticated", None)
    return redirect("/")


@app.route("/api/update-nickname", methods=["POST"])
def update_nickname():
    """ë‹‰ë„¤ì„ ë³€ê²½"""
    try:
        # ë¡œê·¸ì¸ í™•ì¸
        if "user_id" not in session:
            return jsonify({"error": "ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 401

        data = request.json
        new_nickname = data.get("nickname")

        if not new_nickname:
            return jsonify({"error": "ë‹‰ë„¤ì„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        # ë‹‰ë„¤ì„ ê¸¸ì´ ì²´í¬
        if len(new_nickname) < 2 or len(new_nickname) > 20:
            return jsonify({"error": "ë‹‰ë„¤ì„ì€ 2~20ì ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤."}), 400

        # í˜„ì¬ ì‚¬ìš©ì
        user = User.query.get(session["user_id"])
        if not user:
            return jsonify({"error": "ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404

        # í˜„ì¬ ë‹‰ë„¤ì„ê³¼ ê°™ì€ì§€ í™•ì¸
        if user.nickname == new_nickname:
            return jsonify({"error": "í˜„ì¬ ë‹‰ë„¤ì„ê³¼ ë™ì¼í•©ë‹ˆë‹¤."}), 400

        # ë‹‰ë„¤ì„ ì¤‘ë³µ ì²´í¬
        existing_user = User.query.filter_by(nickname=new_nickname).first()
        if existing_user:
            return jsonify({"error": "ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ë‹‰ë„¤ì„ì…ë‹ˆë‹¤."}), 400

        # ë‹‰ë„¤ì„ ë³€ê²½
        old_nickname = user.nickname
        user.nickname = new_nickname
        db.session.commit()

        # ì„¸ì…˜ ì—…ë°ì´íŠ¸
        session["nickname"] = new_nickname

        app.logger.info(f"âœ… ë‹‰ë„¤ì„ ë³€ê²½: {old_nickname} -> {new_nickname}")

        return (
            jsonify(
                {
                    "success": True,
                    "message": "ë‹‰ë„¤ì„ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "nickname": new_nickname,
                }
            ),
            200,
        )

    except Exception as e:
        db.session.rollback()
        app.logger.error(f"ë‹‰ë„¤ì„ ë³€ê²½ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return jsonify({"error": "ë‹‰ë„¤ì„ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


@app.route("/api/my-portfolios", methods=["GET"])
def get_my_portfolios():
    """ë‚´ í¬íŠ¸í´ë¦¬ì˜¤ ëª©ë¡ ì¡°íšŒ"""
    try:
        # ë¡œê·¸ì¸ í™•ì¸
        if "user_id" not in session:
            return jsonify({"error": "ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 401

        import json

        # ì‚¬ìš©ìì˜ í¬íŠ¸í´ë¦¬ì˜¤ ì¡°íšŒ
        portfolios = (
            SavedPortfolio.query.filter_by(user_id=session["user_id"])
            .order_by(SavedPortfolio.last_accessed.desc())
            .all()
        )

        result = []
        for p in portfolios:
            try:
                data = json.loads(p.csv_content)
                metrics = data.get("metrics", {})
                summary = data.get("summary", {})

                result.append(
                    {
                        "id": p.id,
                        "name": p.name,
                        "benchmark": p.benchmark_ticker,
                        "base_currency": p.base_currency,
                        "created_at": p.created_at.strftime("%Y-%m-%d %H:%M"),
                        "last_accessed": p.last_accessed.strftime("%Y-%m-%d %H:%M"),
                        "metrics": metrics,
                        "summary": summary,
                    }
                )
            except Exception as e:
                app.logger.error(f"í¬íŠ¸í´ë¦¬ì˜¤ {p.id} íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        return (
            jsonify({"success": True, "portfolios": result, "count": len(result)}),
            200,
        )

    except Exception as e:
        app.logger.error(f"í¬íŠ¸í´ë¦¬ì˜¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return jsonify({"error": "í¬íŠ¸í´ë¦¬ì˜¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


@app.route("/api/delete-portfolio/<int:portfolio_id>", methods=["DELETE"])
def delete_portfolio(portfolio_id):
    """í¬íŠ¸í´ë¦¬ì˜¤ ì‚­ì œ"""
    try:
        # ë¡œê·¸ì¸ í™•ì¸
        if "user_id" not in session:
            return jsonify({"error": "ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 401

        user_id = session["user_id"]

        # í¬íŠ¸í´ë¦¬ì˜¤ ì¡°íšŒ
        portfolio = SavedPortfolio.query.get(portfolio_id)

        if not portfolio:
            return jsonify({"error": "í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404

        # ì†Œìœ ê¶Œ í™•ì¸ - í•„ìˆ˜!
        if portfolio.user_id != user_id:
            app.logger.warning(
                f"âš ï¸ ê¶Œí•œ ì—†ëŠ” ì‚­ì œ ì‹œë„: User {user_id} -> Portfolio {portfolio_id} (Owner: {portfolio.user_id})"
            )
            return jsonify({"error": "ë³¸ì¸ì˜ í¬íŠ¸í´ë¦¬ì˜¤ë§Œ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."}), 403

        # í¬íŠ¸í´ë¦¬ì˜¤ ì‚­ì œ
        portfolio_name = portfolio.name
        db.session.delete(portfolio)
        db.session.commit()

        app.logger.info(
            f"âœ… í¬íŠ¸í´ë¦¬ì˜¤ ì‚­ì œ ì™„ë£Œ: {portfolio_name} (ID: {portfolio_id}) by User {user_id}"
        )

        return (
            jsonify(
                {
                    "success": True,
                    "message": f"'{portfolio_name}' í¬íŠ¸í´ë¦¬ì˜¤ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
                }
            ),
            200,
        )

    except Exception as e:
        db.session.rollback()
        app.logger.error(f"í¬íŠ¸í´ë¦¬ì˜¤ ì‚­ì œ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return jsonify({"error": "í¬íŠ¸í´ë¦¬ì˜¤ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


@app.route("/api/login", methods=["POST"])
def api_login():
    """ë¡œê·¸ì¸ ì²˜ë¦¬"""
    try:
        data = request.json
        email = data.get("email")
        password = data.get("password")

        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not email or not password:
            return jsonify({"error": "ì´ë©”ì¼ê³¼ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        # ì‚¬ìš©ì ì¡°íšŒ
        user = User.query.filter_by(email=email).first()

        if not user:
            return jsonify({"error": "ì´ë©”ì¼ ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}), 401

        # ì†Œì…œ ë¡œê·¸ì¸ ê³„ì • ì²´í¬
        if user.account_type != "local":
            return (
                jsonify(
                    {
                        "error": f"{user.account_type.upper()} ê³„ì •ì…ë‹ˆë‹¤. í•´ë‹¹ ì†Œì…œ ë¡œê·¸ì¸ì„ ì´ìš©í•´ì£¼ì„¸ìš”."
                    }
                ),
                400,
            )

        # ë¹„ë°€ë²ˆí˜¸ ê²€ì¦
        if not verify_password(password, user.password_hash):
            return jsonify({"error": "ì´ë©”ì¼ ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}), 401

        # í™œì„±í™” ìƒíƒœ í™•ì¸
        if not user.is_active:
            return (
                jsonify({"error": "ë¹„í™œì„±í™”ëœ ê³„ì •ì…ë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."}),
                403,
            )

        # ì„¸ì…˜ì— ì‚¬ìš©ì ì •ë³´ ì €ì¥
        session.clear()  # ê¸°ì¡´ ì„¸ì…˜ í´ë¦¬ì–´
        session["user_id"] = user.id
        session["email"] = user.email
        session["nickname"] = user.nickname
        session["account_type"] = user.account_type
        session.permanent = True  # ì„¸ì…˜ ìœ ì§€ (7ì¼)

        # ë§ˆì§€ë§‰ ë¡œê·¸ì¸ ì‹œê°„ ì—…ë°ì´íŠ¸
        user.last_login = datetime.now()
        db.session.commit()

        app.logger.info(f"âœ… ë¡œê·¸ì¸ ì„±ê³µ: {email} ({user.nickname})")

        return (
            jsonify(
                {
                    "success": True,
                    "message": "ë¡œê·¸ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "user": {
                        "id": user.id,
                        "email": user.email,
                        "nickname": user.nickname,
                        "account_type": user.account_type,
                    },
                }
            ),
            200,
        )

    except Exception as e:
        app.logger.error(f"ë¡œê·¸ì¸ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return jsonify({"error": "ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


@app.route("/api/logout", methods=["POST"])
def api_logout():
    """ë¡œê·¸ì•„ì›ƒ ì²˜ë¦¬"""
    try:
        email = session.get("email", "Unknown")
        session.clear()
        app.logger.info(f"âœ… ë¡œê·¸ì•„ì›ƒ: {email}")

        return jsonify({"success": True, "message": "ë¡œê·¸ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤."}), 200

    except Exception as e:
        app.logger.error(f"ë¡œê·¸ì•„ì›ƒ ì˜¤ë¥˜: {e}")
        return jsonify({"error": "ë¡œê·¸ì•„ì›ƒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


@app.route("/api/me", methods=["GET"])
def get_current_user():
    """í˜„ì¬ ë¡œê·¸ì¸í•œ ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ"""
    try:
        if "user_id" not in session:
            return jsonify({"logged_in": False}), 200

        user = User.query.get(session["user_id"])

        if not user:
            session.clear()
            return jsonify({"logged_in": False}), 200

        return (
            jsonify(
                {
                    "logged_in": True,
                    "user": {
                        "id": user.id,
                        "email": user.email,
                        "nickname": user.nickname,
                        "account_type": user.account_type,
                        "is_admin": user.is_admin,
                    },
                }
            ),
            200,
        )

    except Exception as e:
        app.logger.error(f"ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return jsonify({"error": "ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


@app.route("/api/check-email", methods=["POST"])
def check_email():
    """ì´ë©”ì¼ ì¤‘ë³µ ì²´í¬"""
    try:
        data = request.json
        email = data.get("email")

        if not email:
            return jsonify({"error": "ì´ë©”ì¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        # ì´ë©”ì¼ í˜•ì‹ ê²€ì¦
        import re

        email_pattern = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
        if not re.match(email_pattern, email):
            return jsonify({"error": "ì˜¬ë°”ë¥¸ ì´ë©”ì¼ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤."}), 400

        # ì¤‘ë³µ ì²´í¬
        existing_user = User.query.filter_by(email=email).first()

        if existing_user:
            return (
                jsonify({"exists": True, "message": "ì´ë¯¸ ê°€ì…ëœ ì´ë©”ì¼ì…ë‹ˆë‹¤."}),
                200,
            )

        return jsonify({"exists": False, "message": "ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë©”ì¼ì…ë‹ˆë‹¤."}), 200

    except Exception as e:
        app.logger.error(f"ì´ë©”ì¼ ì²´í¬ ì˜¤ë¥˜: {e}")
        return jsonify({"error": "ì´ë©”ì¼ í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


@app.route("/api/send-verification", methods=["POST"])
def send_verification():
    """ì´ë©”ì¼ ì¸ì¦ë²ˆí˜¸ ì „ì†¡"""
    try:
        data = request.json
        email = data.get("email")

        if not email:
            return jsonify({"error": "ì´ë©”ì¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        # ì´ë¯¸ ê°€ì…ëœ ì´ë©”ì¼ì¸ì§€ ì²´í¬
        existing_user = User.query.filter_by(email=email).first()
        if existing_user:
            return jsonify({"error": "ì´ë¯¸ ê°€ì…ëœ ì´ë©”ì¼ì…ë‹ˆë‹¤."}), 400

        # DEBUG ëª¨ë“œì¸ ê²½ìš° ìë™ ì¸ì¦ ì²˜ë¦¬
        if DEBUG:
            # ê¸°ì¡´ ì¸ì¦ë²ˆí˜¸ ì‚­ì œ
            EmailVerification.query.filter_by(email=email).delete()

            # ìë™ ì¸ì¦ ì™„ë£Œëœ ë ˆì½”ë“œ ìƒì„±
            code = "000000"
            expires_at = datetime.now() + timedelta(minutes=5)
            verification = EmailVerification(
                email=email,
                code=code,
                expires_at=expires_at,
                is_verified=True,  # DEBUG ëª¨ë“œì—ì„œëŠ” ë°”ë¡œ ì¸ì¦ ì™„ë£Œ
            )
            db.session.add(verification)
            db.session.commit()

            app.logger.info(f"ğŸ”§ [DEBUG ëª¨ë“œ] ì´ë©”ì¼ ìë™ ì¸ì¦: {email}")
            return (
                jsonify(
                    {
                        "success": True,
                        "message": "[DEBUG ëª¨ë“œ] ì´ë©”ì¼ ì¸ì¦ì´ ìë™ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                        "expires_in": 300,
                        "debug_mode": True,
                    }
                ),
                200,
            )

        # ê¸°ì¡´ ì¸ì¦ë²ˆí˜¸ ì‚­ì œ (ê°™ì€ ì´ë©”ì¼)
        EmailVerification.query.filter_by(email=email, is_verified=False).delete()

        # ì¸ì¦ë²ˆí˜¸ ìƒì„±
        code = generate_verification_code()
        expires_at = datetime.now() + timedelta(minutes=5)

        # DBì— ì €ì¥
        verification = EmailVerification(email=email, code=code, expires_at=expires_at)
        db.session.add(verification)
        db.session.commit()

        # ì´ë©”ì¼ ì „ì†¡
        if send_verification_email(email, code):
            return (
                jsonify(
                    {
                        "success": True,
                        "message": "ì¸ì¦ë²ˆí˜¸ê°€ ì´ë©”ì¼ë¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.",
                        "expires_in": 300,  # 5ë¶„
                    }
                ),
                200,
            )
        else:
            return jsonify({"error": "ì´ë©”ì¼ ì „ì†¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}), 500

    except Exception as e:
        db.session.rollback()
        app.logger.error(f"ì¸ì¦ë²ˆí˜¸ ì „ì†¡ ì˜¤ë¥˜: {e}")
        return jsonify({"error": "ì¸ì¦ë²ˆí˜¸ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


@app.route("/api/verify-code", methods=["POST"])
def verify_code():
    """ì¸ì¦ë²ˆí˜¸ í™•ì¸"""
    try:
        data = request.json
        email = data.get("email")
        code = data.get("code")

        if not email or not code:
            return jsonify({"error": "ì´ë©”ì¼ê³¼ ì¸ì¦ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        # DEBUG ëª¨ë“œì¸ ê²½ìš° ëª¨ë“  ì¸ì¦ë²ˆí˜¸ í†µê³¼
        if DEBUG:
            # ì´ë¯¸ ì¸ì¦ëœ ë ˆì½”ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
            verification = (
                EmailVerification.query.filter_by(email=email, is_verified=True)
                .order_by(EmailVerification.created_at.desc())
                .first()
            )

            # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            if not verification:
                verification = EmailVerification(
                    email=email,
                    code="000000",
                    expires_at=datetime.now() + timedelta(minutes=5),
                    is_verified=True,
                )
                db.session.add(verification)
                db.session.commit()

            app.logger.info(f"ğŸ”§ [DEBUG ëª¨ë“œ] ì¸ì¦ë²ˆí˜¸ ìë™ í†µê³¼: {email}")
            return (
                jsonify(
                    {
                        "success": True,
                        "message": "[DEBUG ëª¨ë“œ] ì´ë©”ì¼ ì¸ì¦ì´ ìë™ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    }
                ),
                200,
            )

        # ì¸ì¦ë²ˆí˜¸ ì¡°íšŒ
        verification = (
            EmailVerification.query.filter_by(email=email, code=code, is_verified=False)
            .order_by(EmailVerification.created_at.desc())
            .first()
        )

        if not verification:
            return jsonify({"error": "ì˜ëª»ëœ ì¸ì¦ë²ˆí˜¸ì…ë‹ˆë‹¤."}), 400

        # ë§Œë£Œ ì‹œê°„ ì²´í¬
        if datetime.now() > verification.expires_at:
            return (
                jsonify({"error": "ì¸ì¦ë²ˆí˜¸ê°€ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ìš”ì²­í•´ì£¼ì„¸ìš”."}),
                400,
            )

        # ì¸ì¦ ì™„ë£Œ ì²˜ë¦¬
        verification.is_verified = True
        db.session.commit()

        return (
            jsonify({"success": True, "message": "ì´ë©”ì¼ ì¸ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}),
            200,
        )

    except Exception as e:
        db.session.rollback()
        app.logger.error(f"ì¸ì¦ë²ˆí˜¸ í™•ì¸ ì˜¤ë¥˜: {e}")
        return jsonify({"error": "ì¸ì¦ë²ˆí˜¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


@app.route("/api/check-nickname", methods=["POST"])
def check_nickname():
    """ë‹‰ë„¤ì„ ì¤‘ë³µ ì²´í¬"""
    try:
        data = request.json
        nickname = data.get("nickname")

        if not nickname:
            return jsonify({"error": "ë‹‰ë„¤ì„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        # ë‹‰ë„¤ì„ ê¸¸ì´ ì²´í¬
        if len(nickname) < 2 or len(nickname) > 20:
            return jsonify({"error": "ë‹‰ë„¤ì„ì€ 2~20ì ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤."}), 400

        # ì¤‘ë³µ ì²´í¬
        existing_user = User.query.filter_by(nickname=nickname).first()

        if existing_user:
            return (
                jsonify({"exists": True, "message": "ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ë‹‰ë„¤ì„ì…ë‹ˆë‹¤."}),
                200,
            )

        return jsonify({"exists": False, "message": "ì‚¬ìš© ê°€ëŠ¥í•œ ë‹‰ë„¤ì„ì…ë‹ˆë‹¤."}), 200

    except Exception as e:
        app.logger.error(f"ë‹‰ë„¤ì„ ì²´í¬ ì˜¤ë¥˜: {e}")
        return jsonify({"error": "ë‹‰ë„¤ì„ í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


@app.route("/api/signup", methods=["POST"])
def api_signup():
    """íšŒì›ê°€ì… ì²˜ë¦¬"""
    try:
        data = request.json
        email = data.get("email")
        password = data.get("password")
        nickname = data.get("nickname")
        account_type = data.get("account_type", "local")  # ê¸°ë³¸ê°’: local

        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not email or not nickname:
            return jsonify({"error": "ì´ë©”ì¼ê³¼ ë‹‰ë„¤ì„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        # ë¡œì»¬ ê°€ì…ì¸ ê²½ìš° ë¹„ë°€ë²ˆí˜¸ í•„ìˆ˜
        if account_type == "local" and not password:
            return jsonify({"error": "ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        # account_type ê²€ì¦
        if account_type not in ["local", "google", "kakao"]:
            return jsonify({"error": "ì˜¬ë°”ë¥¸ íšŒì› ìœ í˜•ì´ ì•„ë‹™ë‹ˆë‹¤."}), 400

        # ì´ë©”ì¼ ì¸ì¦ í™•ì¸ (ë¡œì»¬ ê°€ì…ë§Œ)
        if account_type == "local":
            verification = (
                EmailVerification.query.filter_by(email=email, is_verified=True)
                .order_by(EmailVerification.created_at.desc())
                .first()
            )

            if not verification:
                return jsonify({"error": "ì´ë©”ì¼ ì¸ì¦ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400

        # ì´ë©”ì¼ ì¤‘ë³µ ì²´í¬ (ì¬í™•ì¸)
        if User.query.filter_by(email=email).first():
            return jsonify({"error": "ì´ë¯¸ ê°€ì…ëœ ì´ë©”ì¼ì…ë‹ˆë‹¤."}), 400

        # ë‹‰ë„¤ì„ ì¤‘ë³µ ì²´í¬ (ì¬í™•ì¸)
        if User.query.filter_by(nickname=nickname).first():
            return jsonify({"error": "ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ë‹‰ë„¤ì„ì…ë‹ˆë‹¤."}), 400

        # ë¹„ë°€ë²ˆí˜¸ í•´ì‹± (ë¡œì»¬ ê°€ì…ë§Œ)
        password_hash = None
        if account_type == "local":
            password_hash = hash_password(password)

        # ì‚¬ìš©ì ìƒì„±
        new_user = User(
            email=email,
            password_hash=password_hash,
            nickname=nickname,
            account_type=account_type,
        )

        db.session.add(new_user)
        db.session.commit()

        app.logger.info(f"âœ… íšŒì›ê°€ì… ì„±ê³µ: {email} ({nickname}) - {account_type}")

        return (
            jsonify(
                {
                    "success": True,
                    "message": "íšŒì›ê°€ì…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "user_id": new_user.id,
                }
            ),
            201,
        )

    except Exception as e:
        db.session.rollback()
        app.logger.error(f"íšŒì›ê°€ì… ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return jsonify({"error": "íšŒì›ê°€ì… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


@app.route("/api/rankings", methods=["GET"])
def get_rankings():
    """í¬íŠ¸í´ë¦¬ì˜¤ ë­í‚¹ ë°ì´í„° ì¡°íšŒ"""
    try:
        import json

        # ë²¤ì¹˜ë§ˆí¬ í•„í„° íŒŒë¼ë¯¸í„° ë°›ê¸°
        benchmark_filter = request.args.get("benchmark", None)

        # ë²¤ì¹˜ë§ˆí¬ ë§¤í•‘ (í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œëª… -> ì‹¤ì œ í‹°ì»¤ë“¤)
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í‹°ì»¤ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬
        benchmark_mapping = {
            "S&P500": ["SPY", "^GSPC"],
            "NASDAQ100": ["QQQ", "^NDX"],
            "KODEX200": ["069500.KS"],
        }

        portfolios = SavedPortfolio.query.all()

        if not portfolios:
            return jsonify(
                {"cagr": [], "sortino": [], "sharpe": [], "alpha": [], "beta": []}
            )

        # í¬íŠ¸í´ë¦¬ì˜¤ ë°ì´í„° íŒŒì‹±
        portfolio_list = []
        for p in portfolios:
            p: SavedPortfolio
            try:
                data = json.loads(p.csv_content)

                # ë²¤ì¹˜ë§ˆí¬ í•„í„°ë§
                if benchmark_filter and benchmark_filter != "ì „ì²´":
                    expected_tickers = benchmark_mapping.get(benchmark_filter, [])
                    # benchmark_tickerê°€ ì—†ê±°ë‚˜ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ
                    if (
                        not p.benchmark_ticker
                        or p.benchmark_ticker not in expected_tickers
                    ):
                        continue

                # ì‚¬ìš©ì ë‹‰ë„¤ì„ ê°€ì ¸ì˜¤ê¸°
                owner_nickname = "ìµëª…"
                if p.user_id:
                    owner = User.query.get(p.user_id)
                    if owner:
                        owner_nickname = owner.nickname

                # ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                summary = data.get("summary", {})
                benchmark_name = summary.get("benchmark_name", p.benchmark_ticker)

                portfolio_list.append(
                    {
                        "id": p.id,
                        "name": p.name,
                        "created_at": p.created_at.isoformat(),
                        "benchmark": p.benchmark_ticker,
                        "benchmark_name": benchmark_name,
                        "owner_nickname": owner_nickname,
                        "metrics": data.get("metrics", {}),
                    }
                )
            except Exception as e:
                print(f"Error parsing portfolio {p.id}: {e}")
                continue

        # ê° ì§€í‘œë³„ Top 5
        cagr_top = sorted(
            portfolio_list,
            key=lambda x: x["metrics"].get("cagr", -999999),
            reverse=True,
        )[:5]
        sortino_top = sorted(
            portfolio_list,
            key=lambda x: x["metrics"].get("sortino_ratio", -999999),
            reverse=True,
        )[:5]
        sharpe_top = sorted(
            portfolio_list,
            key=lambda x: x["metrics"].get("sharpe_ratio", -999999),
            reverse=True,
        )[:5]
        alpha_top = sorted(
            portfolio_list,
            key=lambda x: x["metrics"].get("alpha", -999999),
            reverse=True,
        )[:5]

        # ë² íƒ€ëŠ” 1.0ì— ê°€ê¹Œìš´ ìˆœ
        beta_top = sorted(
            portfolio_list, key=lambda x: abs(x["metrics"].get("beta", 999999) - 1.0)
        )[:5]

        return jsonify(
            {
                "cagr": cagr_top,
                "sortino": sortino_top,
                "sharpe": sharpe_top,
                "alpha": alpha_top,
                "beta": beta_top,
            }
        )

    except Exception as e:
        error_trace = traceback.format_exc()
        print("=" * 80)
        print("âŒ ERROR in /api/rankings endpoint:")
        print(error_trace)
        print("=" * 80)
        return jsonify({"error": str(e)}), 500


@app.route("/portfolio/<int:portfolio_id>")
def view_portfolio(portfolio_id):
    """ì €ì¥ëœ í¬íŠ¸í´ë¦¬ì˜¤ ìƒì„¸ë³´ê¸°"""
    try:
        import json

        portfolio: SavedPortfolio = SavedPortfolio.query.get_or_404(portfolio_id)

        # ë§ˆì§€ë§‰ ì ‘ê·¼ ì‹œê°„ ì—…ë°ì´íŠ¸
        portfolio.last_accessed = datetime.now()
        db.session.commit()

        # ë°ì´í„° íŒŒì‹±
        data = json.loads(portfolio.csv_content)

        # ì‚¬ìš©ì ë‹‰ë„¤ì„ ê°€ì ¸ì˜¤ê¸°
        owner_nickname = "ìµëª…"
        if portfolio.user_id:
            owner = User.query.get(portfolio.user_id)
            if owner:
                owner_nickname = owner.nickname

        # ì†Œìœ ì ì—¬ë¶€ í™•ì¸
        is_owner = False
        if "user_id" in session and portfolio.user_id == session["user_id"]:
            is_owner = True

        # ë¶„ì„ ê²°ê³¼ í˜ì´ì§€ì— ì „ë‹¬
        return render_template(
            "portfolio_view.html",
            portfolio=portfolio,
            data=data,
            owner_nickname=owner_nickname,
            is_owner=is_owner,
        )

    except Exception as e:
        error_trace = traceback.format_exc()
        app.logger.warning("âŒ ERROR in /portfolio/<id> endpoint:")
        app.logger.warning(error_trace)
        return f"í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}", 500


@app.route("/analyze", methods=["POST"])
def analyze():
    """í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„"""
    try:
        # CSV íŒŒì¼ ì½ê¸°
        if "csv_file" not in request.files:
            return jsonify({"error": "CSV íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400

        file = request.files["csv_file"]

        if file.filename == "":
            return jsonify({"error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400

        app.logger.info(f"ğŸ“ Received file: {file.filename}")

        start_date = request.form.get("start_date")
        benchmark_ticker = request.form.get("benchmark_ticker")
        base_currency = request.form.get("base_currency", "USD")

        if not start_date or not benchmark_ticker:
            return jsonify({"error": "ì‹œì‘ ì¼ìì™€ ë²¤ì¹˜ë§ˆí¬ í‹°ì»¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400

        # CSV íŒŒì¼ íŒŒì‹±
        csv_content = file.read().decode("utf-8")
        app.logger.info(f"ğŸ“„ CSV content length: {len(csv_content)} bytes")

        portfolio_df = pd.read_csv(io.StringIO(csv_content))

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ["í‹°ì»¤", "ë³´ìœ ëŸ‰", "êµ­ê°€", "ë¶„ë¥˜"]
        missing_columns = [
            col for col in required_columns if col not in portfolio_df.columns
        ]

        if missing_columns:
            return (
                jsonify(
                    {
                        "error": f'CSV íŒŒì¼ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤: {", ".join(missing_columns)}'
                    }
                ),
                400,
            )

        # ë™ì¼ í‹°ì»¤ ë³‘í•©
        portfolio_df = merge_duplicate_tickers(portfolio_df)

        # ë‚ ì§œ ë³€í™˜
        start_date_obj = datetime.strptime(start_date, "%Y-%m-%d")

        # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê³„ì‚° (ê¸°ì¤€ í†µí™” ì „ë‹¬)
        result = calculate_portfolio_returns(
            portfolio_df, start_date_obj, base_currency
        )

        if result is None or result[0] is None:

            # ì‹¤íŒ¨í•œ í‹°ì»¤ ì •ë³´ ì¶”ì¶œ
            failed_tickers = result[5] if result and len(result) > 5 else []

            error_msg = "í¬íŠ¸í´ë¦¬ì˜¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            if failed_tickers:
                error_msg += (
                    f" ë‹¤ìŒ í‹°ì»¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤: {', '.join(failed_tickers)}"
                )
            else:
                error_msg += " ê°€ëŠ¥í•œ ì›ì¸: 1) ëª¨ë“  í‹°ì»¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ, 2) ì‹œì‘ ë‚ ì§œê°€ ë„ˆë¬´ ìµœê·¼, 3) ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜"

            return jsonify({"error": error_msg}), 400

        (
            portfolio_returns,
            portfolio_series,
            portfolio_data,
            cash_holdings,
            total_initial_value_with_cash,
            failed_tickers,  # ì‹¤íŒ¨í•œ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ë°›ê¸°
        ) = result

        # ì¼ë¶€ í‹°ì»¤ê°€ ì‹¤íŒ¨í•œ ê²½ìš° ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€
        warning_msg = None
        if failed_tickers:
            warning_msg = f"âš ï¸ ë‹¤ìŒ í‹°ì»¤ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(failed_tickers)}"
            app.logger.warning(f"âš ï¸ Warning: Some tickers failed: {failed_tickers}")

        # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        benchmark_returns = None
        benchmark_name = benchmark_ticker  # í‘œì‹œìš© ì´ë¦„

        # í—¤ì§€í€ë“œ ë²¤ì¹˜ë§ˆí¬ì¸ì§€ í™•ì¸
        if benchmark_ticker.startswith("HEDGEFUND_"):
            try:
                app.logger.info(f"ğŸ¢ í—¤ì§€í€ë“œ ë²¤ì¹˜ë§ˆí¬ ì„ íƒ: {benchmark_ticker}")
                benchmark_returns = calculate_hedgefund_benchmark_returns(
                    benchmark_ticker, start_date_obj, base_currency
                )
                benchmark_name = HEDGEFUND_BENCHMARKS[benchmark_ticker]["name"]
                app.logger.info(f"âœ… í—¤ì§€í€ë“œ ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ ì™„ë£Œ: {benchmark_name}")
            except Exception as e:
                app.logger.error(f"âŒ í—¤ì§€í€ë“œ ë²¤ì¹˜ë§ˆí¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
                return (
                    jsonify({"error": f"í—¤ì§€í€ë“œ ë²¤ì¹˜ë§ˆí¬ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}"}),
                    400,
                )
        else:
            # ì¼ë°˜ í‹°ì»¤ ë²¤ì¹˜ë§ˆí¬
            benchmark_data = fetch_stock_data(
                benchmark_ticker, start_date_obj, datetime.now()
            )

            if benchmark_data is None:
                app.logger.warning(
                    f"âŒ Failed to fetch benchmark data for {benchmark_ticker}"
                )
                return (
                    jsonify(
                        {
                            "error": f'ë²¤ì¹˜ë§ˆí¬ í‹°ì»¤ "{benchmark_ticker}"ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‹°ì»¤ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.'
                        }
                    ),
                    400,
                )

            # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë„ fill_missing_dates í˜¸ì¶œ
            benchmark_data = fill_missing_dates(
                benchmark_data, start_date_obj, datetime.now()
            )

            if benchmark_data is None or len(benchmark_data) == 0:
                app.logger.warning(
                    f"âŒ Failed to process benchmark data for {benchmark_ticker}"
                )
                return (
                    jsonify({"error": f"ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}),
                    400,
                )

            # ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥  ê³„ì‚°
            benchmark_returns = benchmark_data.pct_change().dropna()

        # ì§€í‘œ ê³„ì‚°
        metrics = calculate_metrics(portfolio_returns, benchmark_returns)

        if metrics is None:
            app.logger.warning(f"âŒ calculate_metrics returned None")
            return (
                jsonify(
                    {
                        "error": "ì§€í‘œë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬íŠ¸í´ë¦¬ì˜¤ì™€ ë²¤ì¹˜ë§ˆí¬ì˜ ë‚ ì§œ ë²”ìœ„ê°€ ê²¹ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    }
                ),
                400,
            )

        # ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„
        chart_data = prepare_chart_data(
            portfolio_returns, benchmark_returns, portfolio_series
        )

        # í˜„ì¬ í™˜ìœ¨ ì •ë³´ (ë³´ìœ  ì¢…ëª© í…Œì´ë¸”ê³¼ ìš”ì•½ì— ì‚¬ìš©)
        exchange_rate = get_current_exchange_rate()

        # ë„ë„› ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„
        allocation_data = prepare_allocation_data(
            portfolio_data, cash_holdings, total_initial_value_with_cash
        )

        # ë³´ìœ  ì¢…ëª© í…Œì´ë¸” ë°ì´í„° ì¤€ë¹„
        holdings_table = prepare_holdings_table(
            portfolio_data, cash_holdings, base_currency, exchange_rate
        )

        # í¬íŠ¸í´ë¦¬ì˜¤ ìš”ì•½ ì •ë³´
        current_value = portfolio_series.iloc[-1]
        initial_value = portfolio_series.iloc[0]

        # í˜„ê¸ˆ ì´ì•¡ ê³„ì‚°
        total_cash = sum(cash["value"] for cash in cash_holdings.values())

        # í˜„ê¸ˆ í¬í•¨í•œ í˜„ì¬ ì´ ê°€ì¹˜
        current_value_with_cash = current_value + total_cash
        initial_value_with_cash = total_initial_value_with_cash

        summary = {
            "initial_value": round(initial_value, 2),
            "current_value": round(current_value, 2),
            "initial_value_with_cash": round(initial_value_with_cash, 2),
            "current_value_with_cash": round(current_value_with_cash, 2),
            "total_cash": round(total_cash, 2),
            "total_return": round((current_value / initial_value - 1) * 100, 2),
            "total_return_with_cash": round(
                (current_value_with_cash / initial_value_with_cash - 1) * 100, 2
            ),
            "start_date": start_date,
            "end_date": datetime.now().strftime("%Y-%m-%d"),
            "benchmark": benchmark_ticker,
            "benchmark_name": benchmark_name,  # í‘œì‹œìš© ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ ì¶”ê°€
            "num_holdings": len(portfolio_df),
            "base_currency": base_currency,
            "exchange_rate": round(exchange_rate, 2),
        }

        return jsonify(
            {
                "metrics": metrics,
                "chart_data": chart_data,
                "summary": summary,
                "allocation_data": allocation_data,
                "holdings_table": holdings_table,
                "warning": warning_msg,  # ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€
            }
        )

    except Exception as e:
        error_trace = traceback.format_exc()
        app.logger.warning("=" * 80)
        app.logger.warning("âŒ ERROR in /analyze endpoint:")
        app.logger.warning(error_trace)
        app.logger.warning("=" * 80)
        return (
            jsonify(
                {
                    "error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nì„œë²„ í„°ë¯¸ë„ì—ì„œ ìƒì„¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
                }
            ),
            500,
        )


@app.route("/api/ai-analysis", methods=["POST"])
def ai_analysis():
    """OpenAIë¥¼ ì‚¬ìš©í•œ í¬íŠ¸í´ë¦¬ì˜¤ AI ë¶„ì„"""
    try:
        # ë¡œê·¸ì¸ ì²´í¬ - í•„ìˆ˜
        if "user_id" not in session:
            return (
                jsonify({"success": False, "error": "ë¡œê·¸ì¸ì´ í•„ìš”í•œ ê¸°ëŠ¥ì…ë‹ˆë‹¤."}),
                401,
            )

        import json
        from datetime import datetime, timedelta

        # í´ë¼ì´ì–¸íŠ¸ IP ê°€ì ¸ì˜¤ê¸°
        client_ip = request.headers.get("X-Forwarded-For", request.remote_addr)
        if "," in client_ip:
            client_ip = client_ip.split(",")[0].strip()

        current_time = datetime.now()

        # Rate limiting ì²´í¬ (3ë¶„ì— 1ë²ˆ)
        if client_ip in ai_analysis_rate_limit:
            last_request = ai_analysis_rate_limit[client_ip]
            time_diff = (current_time - last_request).total_seconds()

            if time_diff < 180:
                remaining_seconds = int(180 - time_diff)
                return (
                    jsonify(
                        {
                            "success": False,
                            "error": f"AI ë¶„ì„ì€ 3ë¶„ì— 1ë²ˆë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤. {remaining_seconds}ì´ˆ í›„ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                            "rate_limited": True,
                            "remaining_seconds": remaining_seconds,
                        }
                    ),
                    429,
                )

        data: dict = request.json

        # í•„ìš”í•œ ë°ì´í„° ì¶”ì¶œ
        holdings: list = data.get("holdings", [])
        metrics: dict = data.get("metrics", {})
        summary: dict = data.get("summary", {})
        benchmark: str = summary.get("benchmark", "Unknown")

        # ìºì‹œ í‚¤ ìƒì„± (holdingsì˜ í‹°ì»¤ì™€ ë¹„ì¤‘ìœ¼ë¡œ)
        cache_key = json.dumps(
            {
                "holdings": sorted([(h["ticker"], h["weight"]) for h in holdings]),
                "cagr": metrics.get("cagr"),
                "sharpe": metrics.get("sharpe_ratio"),
                "benchmark": benchmark,
            },
            sort_keys=True,
        )

        # ìºì‹œì—ì„œ í™•ì¸ (ê°™ì€ í¬íŠ¸í´ë¦¬ì˜¤ëŠ” ì¬ë¶„ì„í•˜ì§€ ì•ŠìŒ)
        if client_ip in ai_analysis_cache:
            cached_data = ai_analysis_cache[client_ip]
            if cached_data.get("cache_key") == cache_key:
                app.logger.info(f"âœ… Returning cached AI analysis for IP: {client_ip}")
                return jsonify(
                    {"success": True, "analysis": cached_data["result"], "cached": True}
                )

        # ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ ë§¤í•‘
        benchmark_names = {
            "SPY": "S&P 500",
            "^GSPC": "S&P 500",
            "QQQ": "NASDAQ 100",
            "^NDX": "NASDAQ 100",
            "069500.KS": "KOSPI 200",
        }
        benchmark_name = benchmark_names.get(benchmark, benchmark)

        # ë³´ìœ  ì¢…ëª© ì •ë³´ í¬ë§·íŒ…
        holdings_text = "\n".join(
            [f"- {h['ticker']} ({h['name']}): {h['weight']}%" for h in holdings]
        )

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
        You are a professional portfolio analyst. Analyze the following portfolio and produce a structured Korean report.

        ### Portfolio Holdings
        {holdings_text}

        ### Performance Metrics
        - CAGR: {metrics.get('cagr', 'N/A')}%
        - Sharpe Ratio: {metrics.get('sharpe_ratio', 'N/A')}
        - Sortino Ratio: {metrics.get('sortino_ratio', 'N/A')}
        - Alpha: {metrics.get('alpha', 'N/A')}%
        - Beta: {metrics.get('beta', 'N/A')}
        - Volatility: {metrics.get('volatility', 'N/A')}%
        - Max Drawdown (MDD): {metrics.get('max_drawdown', 'N/A')}%

        ### Benchmark ({benchmark_name})
        - Benchmark CAGR: {metrics.get('benchmark_annual_return', 'N/A')}%
        - Benchmark Sharpe: {metrics.get('benchmark_sharpe_ratio', 'N/A')}
        - Benchmark Sortino: {metrics.get('benchmark_sortino_ratio', 'N/A')}
        - Benchmark Volatility: {metrics.get('benchmark_volatility', 'N/A')}%

        ### Output Requirements
        - **Write the response only in Korean**
        - **Do not include any introductory phrases such as "Certainly", "Here is", or similar**
        - Format in **Markdown**
        - Maintain a professional, direct, and objective tone
        - Use appropriate emojis to improve readability
        - The analysis must have exactly three sections with `##` headers:

        1. **í¬íŠ¸í´ë¦¬ì˜¤ ê°•ì  ë¶„ì„**
        - ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„ ê°•ì 
        - ìœ„í—˜ ëŒ€ë¹„ ì„±ê³¼ê°€ ì¢‹ì€ ì´ìœ 
        - êµ¬ì„± ì¸¡ë©´ì˜ ì¥ì 

        2. **í¬íŠ¸í´ë¦¬ì˜¤ ì•½ì  ë° ìœ„í—˜ ìš”ì†Œ**
        - ë¶€ì¡±í•œ ì§€í‘œ
        - ì ì¬ì  ë¦¬ìŠ¤í¬
        - ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„ ì·¨ì•½ ì§€ì 

        3. **ê°œì„  ì œì•ˆ**
        - ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ì „ëµ
        - ë¦¬ë°¸ëŸ°ì‹± ì œì•ˆ
        - í¸ì…/ì œì™¸ ê³ ë ¤ ì¢…ëª©

        Respond in a clear, highly analytical style, with actionable insights and no unnecessary sentences.
        """

        # OpenAI API í˜¸ì¶œ
        app.logger.info("ğŸ¤– Calling OpenAI API...")

        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì „ë¬¸ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê°ê´€ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
            max_tokens=2000,
        )

        analysis_result = response.choices[0].message.content

        app.logger.info("âœ… OpenAI API call successful")

        # Rate limit ì—…ë°ì´íŠ¸
        ai_analysis_rate_limit[client_ip] = current_time

        # ìºì‹œ ì €ì¥
        ai_analysis_cache[client_ip] = {
            "cache_key": cache_key,
            "result": analysis_result,
            "timestamp": current_time,
        }

        return jsonify({"success": True, "analysis": analysis_result, "cached": False})

    except Exception as e:
        error_trace = traceback.format_exc()
        app.logger.warning("âŒ ERROR in /api/ai-analysis endpoint:")
        app.logger.warning(error_trace)
        return (
            jsonify(
                {"success": False, "error": f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
            ),
            500,
        )


if __name__ == "__main__":
    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    init_database()

    # Flask ì•± ì‹¤í–‰ (ì™¸ë¶€ ì ‘ì† í—ˆìš©)
    app.run(host="0.0.0.0", debug=True, port=8000)
